---
layout: post
title: 卷积神经网络详解
categories: [CNN, 面试]
---

## 反向传播

卷积神经网络（Convolutional Neural Network，CNN）的反向传播是指在训练过程中，根据损失函数对网络参数进行更新的过程。反向传播算法是基于链式法则的，通过将损失函数对输出值的偏导数依次向前传递，计算每个神经元的梯度，从而更新参数。

在卷积神经网络中，反向传播算法的实现与全连接神经网络有所不同。**因为卷积层和池化层的参数共享，所以反向传播时需要计算每个参数的梯度并累加到相应位置**。同时，卷积层和池化层也需要计算输入数据的梯度，用于传递到下一层。

### 池化层反向传播

![20230420144751](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230420144751.png)

池化层的反向传播比较容易理解，我们以最大池化举例，上图中，池化后的数字 6 对应于池化前的红色区域，实际上只有红色区域中最大值数字 6 对池化后的结果有影响，权重为 1，而其它的数字对池化后的结果影响都为 0。**假设池化后数字 6 的位置误差为 delta，误差反向传播回去时，红色区域中最大值对应的位置误差即等于 delta，而其它 3 个位置对应的 delta 误差为 0**。

### 卷积层反向传播

虽然卷积神经网络的卷积运算是一个三维张量的图片和一个四维张量的卷积核进行卷积运算，但最核心的计算只涉及二维卷积，因此我们先从二维的卷积运算来进行分析：

![20230420145118](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230420145118.png)

如上图所示，我们求原图A处的delta误差，就先分析，它在前向传播中影响了下一层的哪些结点。显然，它只对结点C有一个权重为B的影响，对卷积结果中的其它结点没有任何影响。因此A的delta误差应该等于C点的delta误差乘上权重B。

![20230420145235](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230420145235.png)

我们现在将原图A点位置移动一下，再看看变换位置后A点的delta误差是多少，同样先分析它前向传播影响了卷积结果的哪些结点。经过分析，**A点以权重C影响了卷积结果的D点，以权重B影响了卷积结果的E点**。那它的delta误差就等于D点delta误差乘上C加上E点的delta误差乘上B。

大家可以尝试用相同的方法去分析原图中其它结点的delta误差，结果会发现，原图的delta误差，等于卷积结果的delta误差经过**零填充**后，**与卷积核旋转180度后的卷积**。如下图所示：

![20230420145408](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230420145408.png)




