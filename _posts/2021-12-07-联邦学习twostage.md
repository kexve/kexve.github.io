---
layout: post 
title: 联邦学习twostage 
categories:  PaperRead   
---   

## 摘要
在本文中，我们设计了一个联邦两阶段学习框架，该框架在设备上增加了一个切割层，并在模型更新中使用基于符号的随机梯度下降和多数投票方法。带有切割层的设备在每一层的基础上分割模型的执行，这有助于在本地学习原始数据(破碎数据)的信息丰富且紧凑的表示。然后，基于来自设备的低维粉碎数据，使用SIGNSGD算法[14]训练全局模型。
### 优点
1. 由于这些紧凑的输入，全局模型需要更少的参数，从而减少了所需的通信中介。
2. 此外，具有[1]多数票的SIGNSGD已被证明可以通过仅传输每个小批随机梯度的符号而在公开的真实数据集上保持竞争结果来缓解通信瓶颈。
3. 提出的模型可以根据数据的来源和分布来处理数据。我们的模型能够处理以前没有观察到的不同数据源，因此适合于一般应用。
4. 然而，我们提出的联邦两阶段学习机制通过最小化粉碎数据和原始数据之间的距离相关性[16]，降低了中间表示的可逆性，同时仍然确保模型的预测精度。

![20211214111033](https://cdn.jsdelivr.net/gh/kexve/img/img/20211214111033.png)

## 相关工作
### 压缩优化
在分布式学习环境中训练大型模型需要较高的通信成本[11]，因此许多压缩优化方法被提出来缓解这一挑战。SIGNSGD试图传递每个小批随机梯度的符号，而不是梯度本身。实验证明，SIGNSGD比传统SGD具有更好的效率。

## 主要内容
为了说明系统的工作原理，我们首先介绍基于符号投票的联邦两阶段学习中的局部操作。然后，我们演示了如何减少全局模型参数和消除潜在的数据泄漏。在展示了全局模型聚合过程并给出了优化算法的伪代码后，描述了如何在系统中使用训练好的模型进行推断。
### 本地split学习
我们通过最小化粉碎数据与原始数据之间的距离相关(DCOR)的对数来找到粉碎数据。我们证明了最小化DCOR可以最小化它们的Kullback-Leibler散度，这是信息论中粉碎数据可逆性的一种度量。为了简单，我们使用距离协方差(DCOV)，它是一个非归一化的DCOR, Kullback-Leibler散度DKL和交叉熵H来建立连接。

![20211214180925](https://cdn.jsdelivr.net/gh/kexve/img/img/20211214180925.png)
![20211214181010](https://cdn.jsdelivr.net/gh/kexve/img/img/20211214181010.png)

### 全局模型聚合

![20211214182749](https://cdn.jsdelivr.net/gh/kexve/img/img/20211214182749.png)

在我们的系统中，我们使用了1位压缩梯度(梯度符号)，大大降低了通信成本。在迭代中，可以根据预测标签和真实标签之间的损失来计算梯度。然后设备将其本地梯度的标志发送回服务器。当服务器从本地设备接收到梯度符号后，通过多数表决方案将这些梯度集合起来，并将1位的决策推回到每个设备。

![20211214183222](https://cdn.jsdelivr.net/gh/kexve/img/img/20211214183222.png)





