---
layout: post
title: 各类梯度下降算法的演化
categories: 深度学习
extMath: true
---

## 各类梯度下降算法的演化

![20220920110848](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220920110848.png)

## 批量梯度下降算法

$$ g = \frac{1}{m} \* \bigtriangledown*{\theta} \sum*{i=1}^{m}L(x^{i},y^{i},\theta) $$  
其中 m 是样本总数。

## 随机梯度下降算法

$$ g = \bigtriangledown*{\theta} \sum*{i=1}^{m}L(x^{i},y^{i},\theta) $$

### 示意图

原始 SGD，学习率(Learning Rate 简称 lr) 过小的话，在有限步内很难走到极小值点。
![20220928113325](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220928113325.png)
原始 SGD，学习率 lr 稍大，就会造成剧烈震荡。表现为训练过程中出现 NaN（Not a Number)，或者训练误差随训练 Epoch 的增加而增加的情况。
![20220928113415](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220928113415.png)
学习率 lr 如果选取合适，SGD 会成功到达极小值点。但想要选中比较合适的学习率，比较困难。这也是为什么大家都很喜欢用对学习率不敏感的梯度下降算法，比如 Adam。在这个例子中，虽然 SGD 初期会沿着最速下降方向（即梯度更大的方向）剧烈震荡，但最终很好的收敛到极小值附近。
![20220928113447](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220928113447.png)

## 小批量梯度下降算法

$$ g = \frac{1}{m'} \* \bigtriangledown*{\theta} \sum*{i=1}^{m'}L(x^{i},y^{i},\theta) $$

## 最速下降法和梯度下降法的区别

梯度下降法：预先设定$\alpha$  
$$ x*{i+1}=x*{i}-\alpha \bigtriangledown f*{x_i} $$
最速下降法：这个步长$\alpha$是通过一个优化函数得到的
$$ \alpha_k=argmin*{\alpha*k}f(x_i-\alpha_k) \bigtriangledown f*{x_i} $$

## Momentum 方法

1.动量方法主要是为了解决 Hessian 矩阵病态条件问题（直观上讲就是梯度高度敏感于参数空间的某些方向）的。  
2.加速学习  
3.一般将参数设为 0.5,0.9，或者 0.99，分别表示最大速度 2 倍，10 倍，100 倍于 SGD 的算法。  
4.通过速度 v，来积累了之间梯度指数级衰减的平均，并且继续延该方向移动：  
$$ \nu \leftarrow \alpha \nu - \epsilon g $$  
可以直观的看到每次的步长为：  
$$ \frac{\epsilon||g||}{1-\alpha} $$  
即当设为 0.5,0.9，或者 0.99，分别表示最大速度 2 倍，10 倍，100 倍于 SGD 的算法（注意，能这样算的前提是，假设 g 保持不变，多轮后 v 的值基本不再变化）。
![20220920165025](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220920165025.png)

### 示意图

不加动量之前，学习率 lr=0.61 时，损失函数已经发散。加入动量之后，同样的学习率，系统却能正确找到极小值点。中等大小的动量，可以帮助系统免疫震荡。
![20220928113602](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220928113602.png)
同样大的学习率，继续增加动量 beta 到 0.81，则会引入新的震荡。<https://distill.pub/> 上有篇研究动量机制的文章，清楚的说明动量会消除某些震荡，也会引入新的震荡。削弱的是小球滚下山坡时沿最速下降方向的震荡，引入的是小球在 V 形谷底附近因惯性导致的左右摇摆。
![20220928113702](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220928113702.png)
一般的做法是选择大的动量 (beta=0.9)，和小的学习率 (此例子中 lr = 0.02)，以使带动量的 SGD 快速稳定收敛。为了在给定步数内成功到达极小值点，我们在在原始的 SGD 中选择 lr=0.42。作为对比，这里加了动量之后的 SGD 的学习率(lr=0.02)要远小于原始 SGD 的 lr。说明动量可以加速收敛。
![20220928113754](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220928113754.png)

## AdaGrad

1.简单来讲，设置全局学习率之后，每次通过，全局学习率逐参数的除以历史梯度平方和的平方根，使得每个参数的学习率不同  
2.效果是：在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小）  
3.缺点是,使得学习率过早，过量的减少  
4.在某些模型上效果不错。  
![20220920163904](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220920163904.png)

### 示意图

Adagrad 需要比较大的学习率才能在给定步内走到极小值点。原因是 Adagrad 是**适配学习率算法**。它对频繁更新的权重使用小的学习率，对比较稀疏，更新较少的权重使用大的学习率。算法累积每个参数的历史梯度平方，将其作为学习率衰减因子。可以看到后期更新点基本重合，意味着更新缓慢。
![20220928113907](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220928113907.png)

## RMSprop

鉴于神经网络都是非凸条件下的，RMSProp 在非凸条件下结果更好，改变梯度累积为指数衰减的移动平均以丢弃遥远的过去历史。  
原始的 RMSprop：  
![20220920164557](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220920164557.png)
结合 Nesterov 动量的 RMSProp：
RMSProp 改变了学习率，Nesterov 引入动量改变了梯度，从两方面改进更新方式。
![20220920164732](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220920164732.png)

### 示意图

RMSProp 算法集成了动量机制与 Adagrad 两种算法的优点。可以看到与 Adagrad 结果相比，RMSProp 的更新点分布更加均匀，可以使用更小的学习率 (lr=0.15) 在给定步到达极小值点，学习率后期衰减不严重。另一方面，与单纯的带动量的 SGD 比，它的更新曲线比较平滑，震荡较小。
![20220928114017](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220928114017.png)

## Adam

1.Adam 算法可以看做是修正后的 Momentum+RMSProp 算法  
2.动量直接并入梯度一阶矩估计中（指数加权）  
3.Adam 通常被认为对超参数的选择相当鲁棒  
4.学习率建议为 0.001  
其实就是 Momentum+RMSProp 的结合，然后再修正其偏差。  
![20220920164926](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220920164926.png)

### 示意图

如果动量可以看成是小球沿斜坡的滚动，那么 Adam 可以理解成带摩擦的铁球，在平地的滚动。

使用 Adam 文章里推荐的默认参数，可以看到就算选择了非常大的学习率 (lr = 1)，Adam 的更新轨迹也只是围绕极小值点附近做 S 形环绕，而不是像原始 SGD，带动量的 SGD 或其他算法一样直接发散。与手动选好学习率的带动量的 SGD 相比，Adam 在极小值点附近表现出复杂的非单调行为。

![20220928114122](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220928114122.png)

减小第一个参数 mu，会缩小极小值附近 S 形曲线的环绕区域，使结果更接近极小值点。这可能是为什么很多文章或开源代码里会选择第一个参数 (mu=0.5)，而不是文章推荐参数 mu=0.9。

![20220928114201](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220928114201.png)

无论 lr 以及 第一个参数如何选择，减小第二个参数 nu, 总会使更新轨迹变得更加曲折。所以最好的选择是使用比较大的 nu (=0.99)。

![20220928115213](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220928115213.png)

固定 mu 和 nu, 无论 lr 选 1 还是 0.35, 系统都能在有限步内走到极小值附近。Adam 表现出良好的特性，一方面对学习率不敏感，另一方面非常稳定。

![20220928114318](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220928114318.png)

很多人都在实践中发现，虽然 Adam 对初始学习率不敏感，训练也比较稳定，但最终能达到的精度没有手动调好的 SGD 来得高。这可能是因为 Adam 喜欢在极小值附近打圈圈吧。如果大家花费了与调 SGD 同等的时间来调 Adam，可能也会得到比较好的精度，就像最后一张图所示的那样（个人愚见）。

## 梯度的一阶矩和二阶矩

`todo`
