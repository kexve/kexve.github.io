---
layout: post
title: 各类梯度下降算法的演化
categories: 深度学习
extMath: true
---

## 各类梯度下降算法的演化
![20220920110848](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220920110848.png)

## 批量梯度下降算法
$$ g = \frac{1}{m} * \bigtriangledown_{\theta} \sum_{i=1}^{m}L(x^{i},y^{i},\theta) $$  
其中m是样本总数。

## 随机梯度下降算法
$$ g = \bigtriangledown_{\theta} \sum_{i=1}^{m}L(x^{i},y^{i},\theta) $$  

## 小批量梯度下降算法
$$ g = \frac{1}{m'} * \bigtriangledown_{\theta} \sum_{i=1}^{m'}L(x^{i},y^{i},\theta) $$  

## 最速下降法和梯度下降法的区别
梯度下降法：预先设定$\alpha$  
$$ x_{i+1}=x_{i}-\alpha \bigtriangledown f_{x_i} $$
最速下降法：这个步长$\alpha$是通过一个优化函数得到的    
$$ \alpha_k=argmin_{\alpha_k}f(x_i-\alpha_k) \bigtriangledown f_{x_i} $$

## Momentum方法
1.动量方法主要是为了解决Hessian矩阵病态条件问题（直观上讲就是梯度高度敏感于参数空间的某些方向）的。  
2.加速学习  
3.一般将参数设为0.5,0.9，或者0.99，分别表示最大速度2倍，10倍，100倍于SGD的算法。  
4.通过速度v，来积累了之间梯度指数级衰减的平均，并且继续延该方向移动：  
$$ \nu \leftarrow \alpha \nu - \epsilon g $$  
可以直观的看到每次的步长为：  
$$ \frac{\epsilon||g||}{1-\alpha} $$  
即当设为0.5,0.9，或者0.99，分别表示最大速度2倍，10倍，100倍于SGD的算法（注意，能这样算的前提是，假设g保持不变，多轮后v的值基本不再变化）。   
![20220920165025](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220920165025.png) 

## AdaGrad
1.简单来讲，设置全局学习率之后，每次通过，全局学习率逐参数的除以历史梯度平方和的平方根，使得每个参数的学习率不同  
2.效果是：在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小）  
3.缺点是,使得学习率过早，过量的减少  
4.在某些模型上效果不错。  
![20220920163904](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220920163904.png)

## RMSprop
鉴于神经网络都是非凸条件下的，RMSProp在非凸条件下结果更好，改变梯度累积为指数衰减的移动平均以丢弃遥远的过去历史。  
原始的RMSprop：  
![20220920164557](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220920164557.png)
结合Nesterov动量的RMSProp：
RMSProp改变了学习率，Nesterov引入动量改变了梯度，从两方面改进更新方式。
![20220920164732](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220920164732.png)

## Adam
1.Adam算法可以看做是修正后的Momentum+RMSProp算法  
2.动量直接并入梯度一阶矩估计中（指数加权）  
3.Adam通常被认为对超参数的选择相当鲁棒  
4.学习率建议为0.001  
其实就是Momentum+RMSProp的结合，然后再修正其偏差。  
![20220920164926](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220920164926.png)

## 梯度的一阶矩和二阶矩
`todo`