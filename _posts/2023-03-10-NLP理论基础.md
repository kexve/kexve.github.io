---
layout: post
title: NLP理论基础
categories: [NLP]
---

## word embeddings
将文本数据转换为数值数据。

词嵌入是自然语言处理（NLP）中语言模型与表征学习技术的统称，它是NLP里的早期预训练技术。它是指**把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中**，每个单词或词组被映射为实数域上的向量，这也是分布式表示：向量的每一维度都没有实际意义，而整体代表一个具体概念。

分布式表示相较于传统的独热编码（one-hot）表示具备更强的表示能力，而独热编码存在维度灾难和语义鸿沟（不能进行相似度计算）等问题。传统的分布式表示方法，如矩阵分解（SVD/LSA）、LDA等均是根据全局语料进行训练，是机器学习时代的产物。

Word Embedding的输入是原始文本中的一组不重叠的词汇，假设有句子：apple on a apple tree。那么为了便于处理，我们可以将这些词汇放置到一个dictionary里，例如：[“apple”, “on”, “a”, “tree”]，这个dictionary就可以看作是Word Embedding的一个输入。

Word Embedding的输出就是每个word的向量表示。对于上文中的原始输入，假设使用最简单的one hot编码方式，那么每个word都对应了一种数值表示。例如，apple对应的vector就是[1, 0, 0, 0]，a对应的vector就是[0, 0, 1, 0]，各种机器学习应用可以基于这种word的数值表示来构建各自的模型。当然，这是一种最简单的映射方法，但却足以阐述Word Embedding的意义。

### 文本表示的类型：

1. 基于one-hot、tf-idf、textrank等的bag-of-words；
2. 主题模型：LSA（SVD）、pLSA、LDA；
3. 基于词向量的固定表征：word2vec、fastText、glove
4. 基于词向量的动态表征：ELMO、GPT、bert

上面给出的4个类型也是nlp领域最为常用的文本表示了，文本是由每个单词构成的，而谈起词向量，one-hot是可认为是最为简单的词向量，但存在**维度灾难和语义鸿沟**等问题；通过构建**共现矩阵并利用SVD求解构建词向量**，则计算复杂度高；而早期词向量的研究通常来源于**语言模型**，比如NNLM和RNNLM，其主要目的是语言模型，而词向量只是一个副产物。

### 使Embedding空前流行的Word2Vec

Word2Vec算法原理：

skip-gram: 用一个词语作为输入，来预测它周围的上下文
cbow: 拿一个词语的上下文作为输入，来预测这个词语本身

![](https://pic1.zhimg.com/v2-df9e2da063fea6ee9431571007c7dee8_r.jpg)
 

`todo`

简单总结一下： Word2Vec 是一个词向量开源工具，包括 Skip-Gram 和 CBOW 的两种算法，加速训练的方法有：Hierarchical Softmax、Sub-Sampling 和 Negative Sampling。

1. Skip-Gram：利用中心词预测上下文；
2. CBOW：利用上下文预测中心词，速度比 Skip-Gram 快；
3. Hierarchical Softmax：引入 Hierarchical 加速 Softmax 的计算过程，对词频低的友好；
4. Sub-Sampling：依据词频进行采样，对词频低的友好；
5. Negative Sampling：通过负采样避免更新全部参数，对词频高的友好； 

### word2vec vs glove

GloVe与word2vec，两个模型都可以根据词汇的“共现co-occurrence”信息，将词汇编码成一个向量（***所谓共现，即语料中词汇一块出现的频率***）。两者最直观的区别在于，word2vec是“**predictive**”的模型，而GloVe是“**count-based**”的模型。具体是什么意思呢？

Predictive的模型，如Word2vec，根据context预测中间的词汇，要么根据中间的词汇预测context，分别对应了word2vec的两种训练方式cbow和skip-gram。对于word2vec，采用**三层神经网络**就能训练，最后一层的输出要**用一个Huffuman树进行词的预测**。

Count-based模型，如GloVe，**本质上是对共现矩阵进行降维**。首先，构建一个词汇的共现矩阵，每一行是一个word，每一列是context。共现矩阵就是计算每个word在每个context出现的频率。由于context是多种词汇的组合，其维度非常大，我们希望像network embedding一样，在context的维度上降维，学习word的低维表示。这一过程可以视为共现矩阵的重构问题，即reconstruction loss。(这里再插一句，降维或者重构的本质是什么？我们选择留下某个维度和丢掉某个维度的标准是什么？Find the lower-dimensional representations which can explain most of the variance in the high-dimensional data，这其实也是PCA的原理)。

### word embedding 搞不定的事

我们知道，多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。然而，Word Embedding针对多义词问题没有得到很好的解决。

![](https://pic3.zhimg.com/v2-21fc157e8c18dc301ccd4c58cd72572e_r.jpg)

### 演进和发展

word embedding得到的词向量是固定表征的，无法解决一词多义等问题，因此引入基于语言模型的动态表征方法：ELMO、GPT、bert，以ELMO为例：

针对多义词问题，ELMO提供了一种简洁优雅的解决方案，ELMO是“Embedding from Language Models”的简称（论文：Deep contextualized word representation）。ELMO的本质思想是：事先用语言模型**学好一个单词的Word Embedding**，此时多义词无法区分，但在实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候可以**根据上下文单词的语义去调整单词的Word Embedding表示**，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。

![](https://pic3.zhimg.com/v2-542e2524b412705c37b10f32ba6258ae_r.jpg)

### graph embedding

基于word2vec的一系列embedding方法主要是基于序列进行embedding，在当前商品、行为、用户等实体之间的关系越来越复杂化、网络化的趋势下，原有sequence embedding方法的表达能力受限，因此Graph Embedding方法的研究和应用成为了当前的趋势。

DeeoWalk (SBU 2014)普遍被认为是Graph Embedding的baseline方法，用极小的代价完成从word2vec到graph embedding的转换和工程尝试；此后提出的LINE(MSRA 2015)，相比DeepWalk纯粹随机游走的序列生成方式，LINE可以应用于有向图、无向图以及边有权重的网络，并通过将一阶、二阶的邻近关系引入目标函数，能够使最终学出的node embedding的分布更为均衡平滑，避免DeepWalk容易使node embedding聚集的情况发生。node2vec (Stanford 2016)对DeepWalk随机游走方式的改进。为了使最终的embedding结果能够表达网络局部周边结构和整体结构，其游走方式结合了深度优先搜索和广度优先搜索。相比于node2vec对游走方式的改进，SDNE模型 (THU 2016)主要从目标函数的设计上解决embedding网络的局部结构和全局结构的问题。而相比LINE分开学习局部结构和全局结构的做法，SDNE一次性的进行了整体的优化，更有利于获取整体最优的embedding。
