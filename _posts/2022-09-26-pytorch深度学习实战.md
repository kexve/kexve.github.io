---
layout: post
title: pytorch深度学习实战
categories: 深度学习
---

## 核心功能
1. **张量**。PyTorch 的核心是一个提供多维数组（张量）以及由 torch
模块提供大量操作的库。张量及操作可以在 CPU 或 GPU 上使用。在 PyTorch 中，将运算从 CPU
转移到 GPU 不需要额外的函数调用。
2. **自动求导引擎**。PyTorch 提供的第 2 个核心功能是张量可以跟踪对其执行的
操作的能力，并分析和计算任何输入对应的输出的导数。该功能用于数值优化，是由张量自身提
供的，通过 PyTorch 底层自动求导引擎来调度。

## 张量
### 张量的本质，张量和list的区别
![20220926154741](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220926154741.png)

### 索引张量
`points[None]`，增加大小为1的维度，就像`unsqueeze()`方法一样

### 命名张量
为了提高效率，PyTorch 根据 NumPy 改编的 einsum()函数指
定了一种索引迷你语言①，为这些乘积的总和的维度提供索引名。在 Python 中，广播（一种用来
概括未命名事物的形式）通常使用三个点（…）来表示，但是请不要担心 einsum()，因为在下面
例子中我们将不会使用它。

张量工厂函数（诸如 tensor()和 rand()函数）有
一个 names 参数，该参数是一个字符串序列。
``` python
# In[7]:
weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])
weights_named
# Out[7]:
tensor([0.2126, 0.7152, 0.0722], names=('channels',))
```

在编写本书时，考虑到这个特性的试验性质，为了避免在索引和对齐方面浪费时间，我们将
在本书剩余部分依然使用未命名维度。命名张量有可能消除对齐错误的许多来源，对齐错误是一件
令人头痛的事情，PyTorch 论坛也许也有所提及。命名张量将被广泛采用，这将是一件有趣的事情。

### 将张量存储到GPU
将通过指定构造函数的相应参数在 GPU 上创建一个张量：

``` python
# In[64]:
points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')
```

我们可以使用 to()方法将在 CPU 上创建的张量复制到 GPU 上：
``` python
# In[65]:
points_gpu = points.to(device='cuda') 
```

请注意，一旦计算出结果，张量 points_gpu 不会返回到 CPU。以下是上述代码产生的
操作：
● 张量 points 被复制到 GPU；
● 在 GPU 上分配一个新的张量，用来存储乘法运算的结果；
● 返回该 GPU 存储的张量的句柄。

没有信息流到 CPU，除非我们输出或访问得到的张量。为了将张
量移回 CPU，我们需要向 to()方法提供一个 cpu 参数，例如：

``` python
# In[69]:
points_cpu = points_gpu.to(device='cpu')
```

### 序列化张量
创建动态张量是很好的，但是如果里面的数据是有价值的，我们将希望将其保存到一个文件
中，并在某个时间加载回来。毕竟，我们不希望每次运行程序时都要从头开始对模型进行训练。
PyTorch 在内部使用 pickle 来序列化张量对象，并为存储添加专用的序列化代码。通过以下方法
可以将张量 points 保存到 ourpoints.t 文件中：
``` python
# In[57]:
torch.save(points, '../data/p1ch3/ourpoints.t')
```
作为替代方法，我们可以传递一个文件描述符来代替文件名：
``` python
# In[58]:
with open('../data/p1ch3/ourpoints.t','wb') as f:
 torch.save(points, f)
```
加载张量 points 同样可以通过一行代码来实现：
``` python
# In[59]:
points = torch.load('../data/p1ch3/ourpoints.t')
```

## 选择线性模型首试
### 问题描述
``` python
# In[2]:
t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]
t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]
t_c = torch.tensor(t_c)
t_u = torch.tensor(t_u)
```
在这里，t_c 值是以摄氏度为单位的温度，而t_u 值是我们未知的单位。我们可以预期2 次测量中的噪声来自设备本身和我们的近似刻度值。为了方便，我们把数据放进了张量中，我们马上就会用到它。
假设模型符合t_c = w * t_u + b
``` python
# In[3]:
def model(t_u, w, b):
 return w * t_u + b 

# In[4]:
def loss_fn(t_p, t_c):
 squared_diffs = (t_p - t_c)**2
 return squared_diffs.mean() 

# In[5]:
w = torch.ones(())
b = torch.zeros(())
t_p = model(t_u, w, b)
t_p
# Out[5]:
tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000,
 21.8000, 48.4000, 60.4000, 68.4000]) 

# In[6]:
loss = loss_fn(t_p, t_c)
loss
# Out[6]:
tensor(1763.8846)
```

### tensor的广播机制

![20220926165556](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220926165556.png)

### 如果不使用梯度下降法
``` python
# In[8]:
delta = 0.1
loss_rate_of_change_w = \
 (loss_fn(model(t_u, w + delta, b), t_c) -
 loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta) 
```
如果变化是负的，那么我们需要增加 w 来最小化损失，而如果变化是正的，我们需要减小 w 的值。

``` python
# In[9]:
learning_rate = 1e-2
w = w - learning_rate * loss_rate_of_change_w

# In[10]:
loss_rate_of_change_b = \
 (loss_fn(model(t_u, w, b + delta), t_c) -
 loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)
b = b - learning_rate * loss_rate_of_change_b
```

### 使用梯度下降法
思想：
``` python
d loss_fn / d w = (d loss_fn / d t_p) * (d t_p / d w)
```

计算导数：
``` python
# In[4]:
def loss_fn(t_p, t_c):
 squared_diffs = (t_p - t_c)**2
 return squared_diffs.mean()

# In[11]:
def dloss_fn(t_p, t_c):
 dsq_diffs = 2 * (t_p - t_c) / t_p.size(0)
 return dsq_diffs 

# In[3]:
def model(t_u, w, b):
 return w * t_u + b

# In[12]:
def dmodel_dw(t_u, w, b):
 return t_u

# In[13]:
def dmodel_db(t_u, w, b):
 return 1.0
```
定义梯度函数：
``` python 
# In[14]:
def grad_fn(t_u, t_c, t_p, w, b):
 dloss_dtp = dloss_fn(t_p, t_c)
 dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)
 dloss_db = dloss_dtp * dmodel_db(t_u, w, b)
 return torch.stack([dloss_dw.sum(), dloss_db.sum()])
```
![20220926171205](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220926171205.png)

### 循环训练
``` python
# In[15]:
def training_loop(n_epochs, learning_rate, params, t_u, t_c):
 for epoch in range(1, n_epochs + 1):
  w, b = params
  t_p = model(t_u, w, b)
  loss = loss_fn(t_p, t_c)
  grad = grad_fn(t_u, t_c, t_p, w, b)
  params = params - learning_rate * grad
  print('Epoch %d, Loss %f' % (epoch, float(loss)))
  return params

# In[18]:
training_loop(
 n_epochs = 100,
 learning_rate = 1e-4,
 params = torch.tensor([1.0, 0.0]),
 t_u = t_u,
 t_c = t_c)
# Out[18]:
Epoch 1, Loss 1763.884644
 Params: tensor([ 0.5483, -0.0083])
 Grad: tensor([4517.2969, 82.6000])
Epoch 2, Loss 323.090546
 Params: tensor([ 0.3623, -0.0118])
 Grad: tensor([1859.5493, 35.7843])
Epoch 3, Loss 78.929634
 Params: tensor([ 0.2858, -0.0135])
 Grad: tensor([765.4667, 16.5122])
...
Epoch 10, Loss 29.105242
 Params: tensor([ 0.2324, -0.0166]) 
 Grad: tensor([1.4803, 3.0544])
Epoch 11, Loss 29.104168
 Params: tensor([ 0.2323, -0.0169])
 Grad: tensor([0.5781, 3.0384])
...
Epoch 99, Loss 29.023582
 Params: tensor([ 0.2327, -0.0435])
 Grad: tensor([-0.0533, 3.0226])
Epoch 100, Loss 29.022669
 Params: tensor([ 0.2327, -0.0438])
 Grad: tensor([-0.0532, 3.0226])
tensor([ 0.2327, -0.0438]) 
```

### 归一化输入
...

### PyTorch 自动求导：反向传播的一切
PyTorch 张量可以记住
它们自己从何而来，根据产生它们的操作和父张量，它们可以根据输入自动提供这些操作的导数
链。这意味着我们不需要手动推导模型，给定一个前向表达式，无论嵌套方式如何，PyTorch 都
会自动提供表达式相对其输入参数的梯度。

``` python
# In[5]:
params = torch.tensor([1.0, 0.0], requires_grad=True)
```
注意到张量构造函数的 requires_grad=True 参数了吗？这个参数告诉 PyTorch 跟踪由对
params 张量进行操作后产生的张量的整个系谱树。换句话说，任何将 params 作为祖先的张量都
可以访问从 params 到那个张量调用的函数链。如果这些函数是可微的（大多数 PyTorch 张量操
作都是可微的），导数的值将自动填充为 params 张量的 grad 属性。

``` python
# In[7]:
loss = loss_fn(model(t_u, *params), t_c)
loss.backward()
params.grad
# Out[7]:
tensor([4517.2969, 82.6000]) 
```