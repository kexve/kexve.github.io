---
layout: post
title: pytorch 深度学习实战
categories: [深度学习, pytorch]
---

## 核心功能

1. **张量**。PyTorch 的核心是一个提供多维数组（张量）以及由 torch
   模块提供大量操作的库。张量及操作可以在 CPU 或 GPU 上使用。在 PyTorch 中，将运算从 CPU
   转移到 GPU 不需要额外的函数调用。
2. **自动求导引擎**。PyTorch 提供的第 2 个核心功能是张量可以跟踪对其执行的
   操作的能力，并分析和计算任何输入对应的输出的导数。该功能用于数值优化，是由张量自身提
   供的，通过 PyTorch 底层自动求导引擎来调度。

## 张量

### 张量的本质，张量和 list 的区别

![20220926154741](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220926154741.png)

### 索引张量

`points[None]`，增加大小为 1 的维度，就像`unsqueeze()`方法一样

### 命名张量

为了提高效率，PyTorch 根据 NumPy 改编的 einsum()函数指
定了一种索引迷你语言 ①，为这些乘积的总和的维度提供索引名。在 Python 中，广播（一种用来
概括未命名事物的形式）通常使用三个点（…）来表示，但是请不要担心 einsum()，因为在下面
例子中我们将不会使用它。

张量工厂函数（诸如 tensor()和 rand()函数）有
一个 names 参数，该参数是一个字符串序列。

```python
# In[7]:
weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])
weights_named
# Out[7]:
tensor([0.2126, 0.7152, 0.0722], names=('channels',))
```

在编写本书时，考虑到这个特性的试验性质，为了避免在索引和对齐方面浪费时间，我们将
在本书剩余部分依然使用未命名维度。命名张量有可能消除对齐错误的许多来源，对齐错误是一件
令人头痛的事情，PyTorch 论坛也许也有所提及。命名张量将被广泛采用，这将是一件有趣的事情。

### 将张量存储到 GPU

将通过指定构造函数的相应参数在 GPU 上创建一个张量：

```python
# In[64]:
points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')
```

我们可以使用 to()方法将在 CPU 上创建的张量复制到 GPU 上：

```python
# In[65]:
points_gpu = points.to(device='cuda')
```

请注意，一旦计算出结果，张量 points_gpu 不会返回到 CPU。以下是上述代码产生的
操作：
● 张量 points 被复制到 GPU；
● 在 GPU 上分配一个新的张量，用来存储乘法运算的结果；
● 返回该 GPU 存储的张量的句柄。

没有信息流到 CPU，除非我们输出或访问得到的张量。为了将张
量移回 CPU，我们需要向 to()方法提供一个 cpu 参数，例如：

```python
# In[69]:
points_cpu = points_gpu.to(device='cpu')
```

### 序列化张量

创建动态张量是很好的，但是如果里面的数据是有价值的，我们将希望将其保存到一个文件
中，并在某个时间加载回来。毕竟，我们不希望每次运行程序时都要从头开始对模型进行训练。
PyTorch 在内部使用 pickle 来序列化张量对象，并为存储添加专用的序列化代码。通过以下方法
可以将张量 points 保存到 ourpoints.t 文件中：

```python
# In[57]:
torch.save(points, '../data/p1ch3/ourpoints.t')
```

作为替代方法，我们可以传递一个文件描述符来代替文件名：

```python
# In[58]:
with open('../data/p1ch3/ourpoints.t','wb') as f:
 torch.save(points, f)
```

加载张量 points 同样可以通过一行代码来实现：

```python
# In[59]:
points = torch.load('../data/p1ch3/ourpoints.t')
```

## 选择线性模型首试

### 问题描述

```python
# In[2]:
t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]
t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]
t_c = torch.tensor(t_c)
t_u = torch.tensor(t_u)
```

在这里，t_c 值是以摄氏度为单位的温度，而 t_u 值是我们未知的单位。我们可以预期 2 次测量中的噪声来自设备本身和我们的近似刻度值。为了方便，我们把数据放进了张量中，我们马上就会用到它。
假设模型符合 t_c = w \* t_u + b

```python
# In[3]:
def model(t_u, w, b):
 return w * t_u + b

# In[4]:
def loss_fn(t_p, t_c):
 squared_diffs = (t_p - t_c)**2
 return squared_diffs.mean()

# In[5]:
w = torch.ones(())
b = torch.zeros(())
t_p = model(t_u, w, b)
t_p
# Out[5]:
tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000,
 21.8000, 48.4000, 60.4000, 68.4000])

# In[6]:
loss = loss_fn(t_p, t_c)
loss
# Out[6]:
tensor(1763.8846)
```

### tensor 的广播机制

![20220926165556](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220926165556.png)

### 如果不使用梯度下降法

```python
# In[8]:
delta = 0.1
loss_rate_of_change_w = \
 (loss_fn(model(t_u, w + delta, b), t_c) -
 loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)
```

如果变化是负的，那么我们需要增加 w 来最小化损失，而如果变化是正的，我们需要减小 w 的值。

```python
# In[9]:
learning_rate = 1e-2
w = w - learning_rate * loss_rate_of_change_w

# In[10]:
loss_rate_of_change_b = \
 (loss_fn(model(t_u, w, b + delta), t_c) -
 loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)
b = b - learning_rate * loss_rate_of_change_b
```

### 使用梯度下降法

思想：

```python
d loss_fn / d w = (d loss_fn / d t_p) * (d t_p / d w)
```

计算导数：

```python
# In[4]:
def loss_fn(t_p, t_c):
 squared_diffs = (t_p - t_c)**2
 return squared_diffs.mean()

# In[11]:
def dloss_fn(t_p, t_c):
 dsq_diffs = 2 * (t_p - t_c) / t_p.size(0)
 return dsq_diffs

# In[3]:
def model(t_u, w, b):
 return w * t_u + b

# In[12]:
def dmodel_dw(t_u, w, b):
 return t_u

# In[13]:
def dmodel_db(t_u, w, b):
 return 1.0
```

定义梯度函数：

```python
# In[14]:
def grad_fn(t_u, t_c, t_p, w, b):
 dloss_dtp = dloss_fn(t_p, t_c)
 dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)
 dloss_db = dloss_dtp * dmodel_db(t_u, w, b)
 return torch.stack([dloss_dw.sum(), dloss_db.sum()])
```

![20220926171205](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220926171205.png)

### 循环训练

```python
# In[15]:
def training_loop(n_epochs, learning_rate, params, t_u, t_c):
 for epoch in range(1, n_epochs + 1):
  w, b = params
  t_p = model(t_u, w, b)
  loss = loss_fn(t_p, t_c)
  grad = grad_fn(t_u, t_c, t_p, w, b)
  params = params - learning_rate * grad
  print('Epoch %d, Loss %f' % (epoch, float(loss)))
  return params

# In[18]:
training_loop(
 n_epochs = 100,
 learning_rate = 1e-4,
 params = torch.tensor([1.0, 0.0]),
 t_u = t_u,
 t_c = t_c)
# Out[18]:
Epoch 1, Loss 1763.884644
 Params: tensor([ 0.5483, -0.0083])
 Grad: tensor([4517.2969, 82.6000])
Epoch 2, Loss 323.090546
 Params: tensor([ 0.3623, -0.0118])
 Grad: tensor([1859.5493, 35.7843])
Epoch 3, Loss 78.929634
 Params: tensor([ 0.2858, -0.0135])
 Grad: tensor([765.4667, 16.5122])
...
Epoch 10, Loss 29.105242
 Params: tensor([ 0.2324, -0.0166])
 Grad: tensor([1.4803, 3.0544])
Epoch 11, Loss 29.104168
 Params: tensor([ 0.2323, -0.0169])
 Grad: tensor([0.5781, 3.0384])
...
Epoch 99, Loss 29.023582
 Params: tensor([ 0.2327, -0.0435])
 Grad: tensor([-0.0533, 3.0226])
Epoch 100, Loss 29.022669
 Params: tensor([ 0.2327, -0.0438])
 Grad: tensor([-0.0532, 3.0226])
tensor([ 0.2327, -0.0438])
```

### 归一化输入

...

### PyTorch 自动求导：反向传播的一切

PyTorch 张量可以记住
它们自己从何而来，根据产生它们的操作和父张量，它们可以根据输入自动提供这些操作的导数
链。这意味着我们不需要手动推导模型，给定一个前向表达式，无论嵌套方式如何，PyTorch 都
会自动提供表达式相对其输入参数的梯度。

```python
# In[5]:
params = torch.tensor([1.0, 0.0], requires_grad=True)
```

注意到张量构造函数的 requires_grad=True 参数了吗？这个参数告诉 PyTorch 跟踪由对
params 张量进行操作后产生的张量的整个系谱树。换句话说，任何将 params 作为祖先的张量都
可以访问从 params 到那个张量调用的函数链。如果这些函数是可微的（大多数 PyTorch 张量操
作都是可微的），导数的值将自动填充为 params 张量的 grad 属性。

```python
# In[7]:
loss = loss_fn(model(t_u, *params), t_c)
loss.backward()
params.grad
# Out[7]:
tensor([4517.2969, 82.6000])
```

![20220926181835](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220926181835.png)

### 累加梯度函数

调用 backward()将导致导数在叶节点上累加。使用梯度进行参数更新后，我们需要显式地将
梯度归零。

```python
# In[8]:
if params.grad is not None:
 params.grad.zero_()
```

### 自动求导代码--实际上就是批量梯度下降算法

```python
# In[9]:
def training_loop(n_epochs, learning_rate, params, t_u, t_c):
 for epoch in range(1, n_epochs + 1):
  if params.grad is not None:
   params.grad.zero_()
  t_p = model(t_u, *params)
  loss = loss_fn(t_p, t_c)
  loss.backward()
  with torch.no_grad():
   params -= learning_rate * params.grad
  if epoch % 500 == 0:
   print('Epoch %d, Loss %f' % (epoch, float(loss)))
  return params
```

注意，params 并不像我们预期的那样简单，其有 2 个特殊性。首先，我们使用 Python 的 with
语句将更新封装在非梯度上下文中，这意味着在 with 块中，PyTorch 自动求导机制将不起作用：
也就是说，不向前向图添加边。实际上，当我们执行这段代码时，PyTorch 记录的前向图在我们
调用 backward()时被消费掉，留下 params 叶节点。但是现在我们想在叶节点建立一个新的前向
图之前改变它。虽然这个例子通常被封装在我们在 5.5.2 小节所讨论的优化器中，但是当我们在
5.5.4 小节中看到 no_grad()的另一种常见用法时我们还会做进一步讨论。

其次，我们在适当的地方更新 params 张量，这意味着我们保持相同的 params 张量但从中减
去更新。当使用自动求导时，我们通常避免就地更新，因为 PyTorch 的自动求导引擎可能需要我
们修改反向传播的值。然而，在这里，我们没有自动求导操作，保持 params 张量是有益的。当
我们在 5.5.2 小节中向优化器注册参数时，不通过为其变量名分配新的张量来替换参数将变得至
关重要。

### 优化器

```python
# In[6]:
params = torch.tensor([1.0, 0.0], requires_grad=True)
learning_rate = 1e-5
optimizer = optim.SGD([params], lr=learning_rate)

# In[7]:
t_p = model(t_u, *params)
loss = loss_fn(t_p, t_c)

optimizer.zero_grad()

loss.backward()
optimizer.step()

params

# Out[7]:
tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)
```

params 的值在调用 step()时更新，而不需要我们自己去操作！即优化器会查看 params.grad
并更新 params，从中减去学习率乘梯度，就像我们以前手动编写的代码一样。

同样的，也可以写出训练循环。

### 自动求导更新及关闭

使用相关的 set_grad_enabled()，我们还可以根据一个布尔表达式设定代码运行时启用或禁用
自动求导的条件，典型的条件是我们是在训练模式还是推理模式下运行。例如，我们可以定义一
个 calc_forward()方法，它接收数据作为输入，根据一个布尔类型的参数决定 model()和 loss_fn()是否会进行自动求导。

```python
# In[17]:
def calc_forward(t_u, t_c, is_train):
 with torch.set_grad_enabled(is_train):
  t_p = model(t_u, *params)
  loss = loss_fn(t_p, t_c)
 return loss
```

## pytorch nn 模块

回到我们的线性模型，构造函数 nn.Linear 接收 3 个参数：输入特征的数量、输出特征的数
量，以及线性模型是否包含偏置（这里默认是 True）。

```python
# In[5]:
import torch.nn as nn
linear_model = nn.Linear(1, 1)
linear_model(t_un_val)
# Out[5]:
tensor([[0.6018],
 [0.2877]], grad_fn=<AddmmBackward>)

# In[6]:
linear_model.weight
# Out[6]:
Parameter containing:
tensor([[-0.0674]], requires_grad=True)
# In[7]:
linear_model.bias
# Out[7]:
Parameter containing:
tensor([0.7488], requires_grad=True)
```

### 批量输入

nn 中的所有模块都被编写为可以同时为多个输入产生输出。因此，假设我们需要在 10 个样
本上运行 nn.Linear，我们可以创建一个大小为 B×Nin 的输入张量，其中 B 是批次的大小，Nin
为输入特征的数量，并在模型上运行一次。例如：

```python
# In[9]:
x = torch.ones(10, 1)
linear_model(x)
# Out[9]:
tensor([[0.6814],
 [0.6814],
 [0.6814],
 [0.6814],
 [0.6814],
 [0.6814],
 [0.6814],
 [0.6814],
 [0.6814],
 [0.6814]], grad_fn=<AddmmBackward>)
```

让我们深入了解一下这里发生了什么，图 6.7 显示了批处理图像数据的类似情况。我们输入
的是 B×C×H×W，批次大小为 3（例如，狗、鸟和汽车的图像）、3 个通道维度（红、绿、蓝），
高度和宽度的像素数量未指定的图像。我们可以看到，输出是一个大小为 B×Nout 的张量，其中
Nout 是输出特征的数量：在本例中为 4。
![20220926191153](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220926191153.png)

我们要进行批处理的原因是多方面的。一个很大的目的是确保我们要求的计算足够大，以饱
和我们用于执行计算的计算资源。特别是 GPU 是高度并行化的，因此一个小模型上的单个输入
将使大部分单元处于空闲状态。通过提供批量输入，计算可以分散到其他空闲的单元中，这就意
味着批量的结果返回的速度与单个结果返回的速度一样快。另一个目的是，一些高级模型使用来
自整个批处理的统计信息，并且随着批处理大小的增加，这些统计信息会变得更好。

```python
# In[2]:
t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]
t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]
t_c = torch.tensor(t_c).unsqueeze(1)
t_u = torch.tensor(t_u).unsqueeze(1)
t_u.shape
# Out[2]:
torch.Size([11, 1])

# In[10]:
linear_model = nn.Linear(1, 1)
optimizer = optim.SGD(
 linear_model.parameters(),
 lr=1e-2)

# In[11]:
linear_model.parameters()
# Out[11]:
<generator object Module.parameters at 0x7f94b4a8a750>
# In[12]:
list(linear_model.parameters())
# Out[12]:
[Parameter containing:
 tensor([[0.7398]], requires_grad=True), Parameter containing:
 tensor([0.7974], requires_grad=True)]

# In[13]:
def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val,
 t_c_train, t_c_val):
 for epoch in range(1, n_epochs + 1):
  t_p_train = model(t_u_train)
  loss_train = loss_fn(t_p_train, t_c_train)
  t_p_val = model(t_u_val)
  loss_val = loss_fn(t_p_val, t_c_val)
  optimizer.zero_grad()
  loss_train.backward()
  optimizer.step()
  if epoch == 1 or epoch % 1000 == 0:
   print(f"Epoch {epoch}, Training loss {loss_train.item():.4f},"
   f" Validation loss {loss_val.item():.4f}")
```

最后一点，我们可以利用来自 torch.nn 的损失。实际上，nn 包含几个常见的损失函数，其中包括 nn.MSELoss()（MSE 代表均方误差），这正是我们之前定义的 loss_fn()。nn 中的损失函数仍然是 nn.Module 的子类，因此我们将创建一个实例并将其作为函数调用。在本例中，我们去掉了自定义的 loss_fn()函数，并用 nn.MSELoss()替换了它。

```python
# In[15]:
linear_model = nn.Linear(1, 1)
optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)
training_loop(
 n_epochs = 3000,
 optimizer = optimizer,
 model = linear_model,
 loss_fn = nn.MSELoss(),
 t_u_train = t_un_train,
 t_u_val = t_un_val,
 t_c_train = t_c_train,
 t_c_val = t_c_val)
print()
print(linear_model.weight)
print(linear_model.bias)
# Out[15]:
Epoch 1, Training loss 134.9599, Validation loss 183.1707
Epoch 1000, Training loss 4.8053, Validation loss 4.7307
Epoch 2000, Training loss 3.0285, Validation loss 3.0889
Epoch 3000, Training loss 2.8569, Validation loss 3.9105
Parameter containing:
tensor([[5.4319]], requires_grad=True)
Parameter containing:
tensor([-17.9693], requires_grad=True)
```

### 替换线性模型

nn 提供了一种通过 nn.Sequential 容器来连接模型的方式：

```python
# In[13]:
def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val,
 t_c_train, t_c_val):
 for epoch in range(1, n_epochs + 1):
  t_p_train = model(t_u_train)
  loss_train = loss_fn(t_p_train, t_c_train)
  t_p_val = model(t_u_val)
  loss_val = loss_fn(t_p_val, t_c_val)
  optimizer.zero_grad()
  loss_train.backward()
  optimizer.step()
  if epoch == 1 or epoch % 1000 == 0:
   print(f"Epoch {epoch}, Training loss {loss_train.item():.4f},"
   f" Validation loss {loss_val.item():.4f}")

# In[16]:
seq_model = nn.Sequential(
 nn.Linear(1, 13),
 nn.Tanh(),
 nn.Linear(13, 1))
seq_model
# Out[16]:
Sequential(
 (0): Linear(in_features=1, out_features=13, bias=True)
 (1): Tanh()
 (2): Linear(in_features=13, out_features=1, bias=True)
)

# In[22]:
optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)
training_loop(
 n_epochs = 5000,
 optimizer = optimizer,
 model = seq_model,
 loss_fn = nn.MSELoss(),
 t_u_train = t_un_train,
 t_u_val = t_un_val,
 t_c_train = t_c_train,
 t_c_val = t_c_val)
print('output', seq_model(t_un_val))
print('answer', t_c_val)
print('hidden', seq_model.hidden_linear.weight.grad)
# Out[22]:
Epoch 1, Training loss 182.9724, Validation loss 231.8708
Epoch 1000, Training loss 6.6642, Validation loss 3.7330
Epoch 2000, Training loss 5.1502, Validation loss 0.1406
Epoch 3000, Training loss 2.9653, Validation loss 1.0005
Epoch 4000, Training loss 2.2839, Validation loss 1.6580
Epoch 5000, Training loss 2.1141, Validation loss 2.0215
output tensor([[-1.9930],
 [20.8729]], grad_fn=<AddmmBackward>)
answer tensor([[-4.],
 [21.]])
hidden tensor([[ 0.0272],
 [ 0.0139],
 [ 0.1692],
 [ 0.1735],
 [-0.1697],
 [ 0.1455],
 [-0.0136],
 [-0.0554]])
```

## 解决一个简单的分类问题

### dataloader 和 dataset 对象

...

### 数据预处理--归一化

变换非常方便，因为我们可以使用 transforms.Compose()将它们连接起来，然后在数据加载
器中直接透明地进行数据归一化和数据增强操作。例如，一种好的做法是对数据进行归一化，使
每个通道的均值为 0，标准差为 1。我们在第 4 章中提到过这一点，但是现在，读完第 5 章之后，
我们有了一个直觉：**通过选择在 0±1（或 2）附近呈线性的激活函数，将数据保持在相同的范围
内意味着神经元更有可能具有非零梯度，因此，可以更快地学习**。同时，**对每个通道进行归一化，
使其具有相同的分布，可以保证在相同的学习率下，通过梯度下降实现通道信息的混合和更新**。
这就像 5.4.4 小节中我们将权重重新调整为与温度转换模型中的偏置相同量级的情况。

```python
# In[15]:
imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)
imgs.shape
# Out[15]:
torch.Size([3, 32, 32, 50000])
现在我们可以很容易地计算出每个信道的平均值：
# In[16]:
imgs.view(3, -1).mean(dim=1)
# Out[16]:
tensor([0.4915, 0.4823, 0.4468])
计算标准差也是类似的：
# In[17]:
imgs.view(3, -1).std(dim=1)
# Out[17]:
tensor([0.2470, 0.2435, 0.2616])
有了这些数字，我们就可以初始化 Normalize 变换了：
# In[18]:
transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))
# Out[18]:
Normalize(mean=(0.4915, 0.4823, 0.4468), std=(0.247, 0.2435, 0.2616))

并将其连接到 ToTensor 变换：
# In[19]:
transformed_cifar10 = datasets.CIFAR10(
 data_path, train=True, download=False,
transform=transforms.Compose([
 transforms.ToTensor(),
 transforms.Normalize((0.4915, 0.4823, 0.4468),
 (0.2470, 0.2435, 0.2616))
]))
```

### 构建一个全连接

```python
# In[6]:
import torch.nn as nn
n_out = 2
model = nn.Sequential(
 nn.Linear(
 3072,
 512,
 ),
 nn.Tanh(),
 nn.Linear(
 512,
 n_out,
 )
 )

现在我们可以在模型的末尾添加一个 nn.Softmax()，这样我们的网络就可以产生概率了。
# In[11]:
model = nn.Sequential(
 nn.Linear(3072, 512),
 nn.Tanh(),
 nn.Linear(512, 2),
 nn.Softmax(dim=1))
```

### 分类的损失

然而，仔细
想想，我们并不是真的对精确地再现这些值感兴趣。回头看看我们用来提取预测类索引的
argmax 操作，我们真正感兴趣的是，对于飞机来说，第 1 个概率高于第 2 个，对于鸟类来说，
第 2 个概率也高于第 1 个概率。换句话说，我们希望惩罚错误分类，而不是煞费苦心地惩罚
那些看起来不完全像 0.0 或 1.0 的东西。
在这种情况下，我们需要最大化的是与正确的类 out[class_index]相关的概率，其中 out 是
softmax 的输出，class_index 是一个向量，对于每个样本，“飞机”为 0，“鸟”为 1。这个值，即与
正确的类别相关的概率，被称为我们的模型给定参数的似然 ①。换句话说，我们想要一个损失函数，
当概率很低的时候，损失非常高：低到其他选择都有比它更高的概率。相反，当概率高于其他选
择时，损失应该很低，而且我们并不是真的专注于将概率提高到 1。

综上所述，对于批次中的每个样本，我们的分类损失可以按如下步骤计算。  
（1）运行正向传播，并从最后的线性层获得输出值。  
（2）计算它们的 Softmax，并获得概率。  
（3）取与目标类别对应的预测概率（参数的可能性）。请注意，我们知道目标类别是什么，
这是一个有监督的问题，这是基本事实。  
（4）计算它的对数，在它前面加上一个负号，再添加到损失中。

然后在给定一批数据的情况下计算模型的 NLL，输
入约定背后的原因是当概率接近 0 时，取概率的对数是很棘手的事情。解决方法是使用
nn.LogSoftmax()而不是使用 nn.Softmax()，以确保计算在数字上稳定。
我们现在可以修改模型，使用 nn.LogSoftmax()作为输出模块：

```python
model = nn.Sequential(
 nn.Linear(3072, 512),
 nn.Tanh(),
 nn.Linear(512, 2),
 nn.LogSoftmax(dim=1))
然后我们实例化 NLL 损失：
loss = nn.NLLLoss()
损失将批次的 nn.LogSoftmax()的输出作为第 1 个参数，将类别索引的张量（在我们的例子中是 0 和 1）作为第 2 个参数。我们现在可以用鸟来测试它。
img, label = cifar2[0]
out = model(img.view(-1).unsqueeze(0))
loss(out, torch.tensor([label]))
tensor(0.6509, grad_fn=<NllLossBackward>)
```

_我们来看看如何使用交叉熵损失来改进 MSE 以结束我们对损失的研究。在图 7.11 中，我们
可以看到，当预测偏离目标时，交叉熵损失有一定的斜率（在低损失角落，目标类别的预测概率
约为 99.97%），虽然我们在一开始就忽略了 MSE，但它在更早的时候就饱和了，而且关键的是它
也存在非常错误的预测。根本原因是 MSE 的斜率太低，无法补偿 Softmax 函数对于错误预测的
平坦度。这就是为什么 MSE 不适合分类工作。_

![20220927152251](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220927152251.png)

### 训练

通过在每个迭代周期上变换样本并一次估计一个或几个样本的梯度（为了稳定性
更高），我们在梯度下降中有效地引入了随机性。还记得 SGD 吗？S 的含义就是处理小批量（又
名 minibatch）的、打乱的数据。事实证明，在小批量上估计的梯度是在整个数据集上估计的梯
度的较差近似值，有助于收敛并防止优化在过程中陷入局部极小。如图 7.13 所示，小批量上的梯
度随机偏离理想轨迹，这也是我们要使用一个相当小的学习率的部分原因。

###

torch.utils.data 模块有一个 DataLoader 类，该类有助于打乱数据和组织数据。数据加载器的工作是从数据集中采样小批量，这使我们能够灵活地选择不同的采样策略。一种非常常见的策略是在每个迭代周期洗牌后进行均匀采样。图 7.14 显示了数据加载器对它从数据集获得的索引进行洗牌的过程。
![20220927153235](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220927153235.png)
让我们看看这是怎么做的。DataLoader()构造函数至少接收一个数据集对象作为输入，以
及 batch_size 和一个 shuffle 布尔值，该布尔值指示数据是否需要在每个迭代周期开始时被重
新打乱：

```python
train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,
 shuffle=True)
```

DataLoader 可以被迭代，因此我们可以直接在新训练代码的内部循环中使用它：

```python
import torch
import torch.nn as nn

# 在训练集上训练
train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,
 shuffle=True)
model = nn.Sequential(
 nn.Linear(3072, 512),
 nn.Tanh(),
 nn.Linear(512, 2),
 nn.LogSoftmax(dim=1))
learning_rate = 1e-2
optimizer = optim.SGD(model.parameters(), lr=learning_rate)
loss_fn = nn.NLLLoss()
n_epochs = 100
for epoch in range(n_epochs):
 for imgs, labels in train_loader:
 batch_size = imgs.shape[0]
 outputs = model(imgs.view(batch_size, -1))
 loss = loss_fn(outputs, labels)
 optimizer.zero_grad()
 loss.backward()
 optimizer.step()
print("Epoch: %d, Loss: %f" % (epoch, float(loss)))

# 在验证集上验证
val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,
 shuffle=False)
correct = 0
total = 0
with torch.no_grad():
 for imgs, labels in val_loader:
 batch_size = imgs.shape[0]
 outputs = model(imgs.view(batch_size, -1))
 _, predicted = torch.max(outputs, dim=1)
 total += labels.shape[0]
 correct += int((predicted == labels).sum())
print("Accuracy: %f", correct / total)
Accuracy: 0.794000
```

nn.LogSoftmax()和 nn.NLLLoss()的组合相当于使用 nn.CrossEntropyLoss()。交叉熵损失函数
是 PyTorch 的一个特性，实际上 nn.NLLLoss()计算的是交叉熵，但将对数概率预测作为输入，其
中 nn.CrossEntropyLoss()用于计算分数（有时称为 logits）。从技术上讲，nn.NLLLoss()计算的是
把所有质量放在目标上的狄拉克分布和由对数概率输入给出的预测分布之间的交叉熵。

从神经网络中丢弃最后一个 nn.LogSoftmax()，转而使用 nn.CrossEntropyLoss()是很常见的。
让我们试试：

```python
model = nn.Sequential(
 nn.Linear(3072, 1024),
 nn.Tanh(),
 nn.Linear(1024, 512),
 nn.Tanh(),
 nn.Linear(512, 128),
 nn.Tanh(),
 nn.Linear(128, 2))
loss_fn = nn.CrossEntropyLoss()
```

### 全连接网络处理图片的局限

...

## 使用卷积进行泛化

### 卷积提供局部性和平移不变性、模型的参数大幅减少

为了实现这个目标，我们在第 7
章中使用的图像（向量矩阵）需要实现一个相当复杂的权重模式。**大多数权重矩阵将为 0，因为
对于与输入像素对应的项，由于距离输出像素太远而不能产生影响。对于其他权重矩阵，我们必
须找到一种方法，使输入和输出像素的相对位置保持同步。这意味着我们需要将它们初始化为相
同的值，并确保在训练期间网络更新时所有绑定权重保持不变**。这样，我们就可以确保权重在邻
域中操作以响应局部模式，并且无论局部模式出现在图像的哪个位置，都可以被识别出来。

![20220927155144](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220927155144.png)

现在我们可以看到它与我们之前提到的内容的联系了：卷积等价于进行多重线性操作，它们
的权重几乎在除个别像素外的任何地方都为 0，并且在训练期间接收相同的更新。

### 卷积实战

torch.nn 模块提供一维、二维、
三维的卷积，其中 nn.Conv1d 用于时间序列，nn.Conv2d 用于图像，nn.Conv3d 用于体数据和视频。

提供给 nn.Conv2d 的参数至少包括输入特
征（或通道，因为我们处理的是多通道图像，也就是说，每个像素有多个值）的数量、输出特征
的数量以及核的大小等。例如，对于第 1 个卷积模块，每个像素有 3 个输入特征（RGB 通道），
输出特征具有任意数量的通道数，如有 16 个通道。输出图像的通道越多，表示网络的容量越大，
我们借助这些通道能够检测到许多不同类型的特征。另外，由于我们是随机对它们进行初始化的，
因此我们得到的一些特征，即使经过训练，也是无用的 ①。让我们使用 3×3 的卷积核。

```python
# In[11]:
conv = nn.Conv2d(3, 16, kernel_size=3)
conv
# Out[11]:
Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))
```

权重张量的形状应该是什么样的呢？卷积核的大小为 3×3，所以我们希望权重由 3×3 个部分
组成。对于单个输出像素值，我们的卷积核考虑有 in_ch=3 个输入通道，因此对于单个输出像素
值，其权重分量（平移整个输出通道的不变量）为 in_ch×3×3。最后，我们有和输出通道一样多
的通道，这里输出通道 out_ch 有 16 个，所以完整的权重张量是 out_ch×in_ch×3×3，在我们的例子中是
16×3×3×3。偏置张量的大小为 16，为了简单，我们暂时还没有讨论过偏置，但是就像线性模块
的情况一样，它是我们添加到输出图像的每个通道上的一个常量值。让我们验证一下我们的假设：

```python
# In[12]:
conv.weight.shape, conv.bias.shape
# Out[12]:
(torch.Size([16, 3, 3, 3]), torch.Size([16]))
```

一个二维卷积产生一个二维图像并将其作为输出，它的像素是输入图像邻域的加权和。在我
们的例子中，核权重和偏置都是随机初始化的，因此输出的图像不会特别有意义。和往常一样，
如果我们想用一个输入图像调用 conv 模块，我们需要通过 unsqueeze()添加第 0 批处理维度，因
为 nn.Conv2d()期望输入一个 B×C×H×W 的张量：

```python
# In[13]:
img, _ = cifar2[0]
output = conv(img.unsqueeze(0))
img.unsqueeze(0).shape, output.shape
# Out[13]:
(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))
```

![20220927162053](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220927162053.png)

### 填充边界

默认情况下，PyTorch 将在输入图像中滑动卷积核，得到
width−kernel_width + 1 个水平和垂直位置。**对于奇数大小的卷积核，这将导致图像的宽度是卷积核
宽度的一半（在我们的例子中，3//2 = 1），这就解释了为什么我们在每个维度上都少了 2 个像素。比如，一个 200*200 像素的图像，经过 10 层 5*5 的卷积后，将减少到 200\*200 像素。**

**_卷积神经网络中卷积核的高度和宽度通常为奇数，例如 1、3、5 或 7。 选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。_**

然而，PyTorch 为我们提供了一种填充图像的可能性—通过在边界周围创建重影像素
（ghost pixel）来填充图像。就卷积而言，这些重影像素的值为 0。图 8.3 显示了填充操作。

![20220927162626](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220927162626.png)

在我们的例子中，当 kernel_size=3 时指定 padding=1 意味着在 i00 位置的上面和左边有一组
额外的领域，这样即使在原始图像的角落也可以计算出卷积的输出 ①。最终的结果是，输出图像
与输入图像的大小完全相同。

将卷积核应用于我们的图像，可看到图 8.5 所示的结果。正如预期的那样，卷积核增强了垂
直边缘。我们可以构建更复杂的过滤器，例如检测水平或对角线边缘、十字或棋盘图案，其中
“检测”意味着输出具有较高的峰值（high magnitude）。事实上，计算机视觉专家的工作一直以
来就是设计出最有效的过滤器组合，使图像中突出的某些特征和物体能够被识别出来。
![20220927165659](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220927165659.png)

通过深度学习，我们让核函数从数据中估计，以辨别最有效的方式：例如，在最小化输出与
我们在 7.2.5 小节中介绍的实际数据之间的负交叉熵损失。从这个角度来看，卷积神经网络的工
作是估计连续层中的一组过滤器的卷积核，这些过滤器将把一个多通道图像转换成另一个多通道
图像，其中不同的通道对应不同的特征，例如一个通道代表平均值，一个通道代表垂直边缘，等
等。图 8.6 显示了卷积学习的过程。

![20220927165911](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220927165911.png)

### 深度和池化技术

这一切看起来都很好，但其实还有一些问题。我们都很兴奋，因为通过从全连接层移动到卷
积，我们实现了局部性和平移不变性。然后我们建议使用小卷积核，如 3×3 或 5×5，以实现峰值
局部性。整体情况如何呢？我们如何知道图像中的所有结构是 3 像素还是 5 像素宽呢？我们没办
法知道，因为它们也不是。如果它们不是，那么我们的网络将如何设置以看到更大范围的图像呢？
如果我们想有效地识别鸟和飞机，那么这正是我们真正需要解决的问题，因为尽管 CIFAR-10 图
像很小，但仍有对象跨越几个像素，如翅膀或机翼。

一种方法是使用大的卷积核。当然，对于一个 32×32 的图像，我们最大可以使用 32×32 的卷
积核，但是我们会收敛到旧的全连接层，仿射变换失去了卷积所有优良性质。卷积神经网络中使用
的另一种方法是在一个卷积之后堆叠另一个卷积，同时在连续卷积之间对图像进行下采样。

1. 取 4 个像素的平均值。这种平均池化在早期是一种常见的方法，但现在已经不受欢迎了。
1. 取 4 个像素的最大值。这种方法称为最大池化（max pooling），是目前最常用的方法之一，
   但它有丢弃剩余四分之三的数据的缺点。
1. 使用带步长的卷积。该方法只将每第 N 个像素纳入计算。步长为 2 的 3×4 卷积仍然包含
   来自前一层所有像素的输入。文献显示了这种方法的前景，但它还没有取代最大池化法。

```python
# 最大池化
# In[21]:
pool = nn.MaxPool2d(2)
output = pool(img.unsqueeze(0))
img.unsqueeze(0).shape, output.shape
# Out[21]:
(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))
```

### 卷积和下采样结合

![20220927170501](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220927170501.png)

因此，第 1 组卷积核对一阶、低级特征的小邻域进行操作，而第 2 组卷积核则有效地对更宽
的邻域进行操作，生成由先前特征组成的特征。这是一种非常强大的机制，它为卷积神经网络提
供了查看非常复杂场景的能力—比我们从 CIFAR-10 数据集获得的 32×32 的图像要复杂得多。

### 整合起来

![20220927171052](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220927171052.png)

```python
# In[23]:
model = nn.Sequential(
 nn.Conv2d(3, 16, kernel_size=3, padding=1),
 nn.Tanh(),
 nn.MaxPool2d(2),
 nn.Conv2d(16, 8, kernel_size=3, padding=1),
 nn.Tanh(),
 nn.MaxPool2d(2),
 # ... 这里缺少了一些重要的东西
 nn.Linear(8 * 8 * 8, 32),
 nn.Tanh(),
 nn.Linear(32, 2))
```

这里缺少的是从有 8 个通道的、8×8 的图像转换为有 512 个元素的一维向量的步骤（如果我们
忽略批处理维度，则为一维向量）。这可以通过对最后一个 nn.MaxPool2d()的输出调用 view()来实现。
但不幸的是，在使用 nn.Sequential 时，我们没有以任何显式可见的方式展示每个模块的输出 ①。最近，PyTorch 获得了一个 nn.Flatten 层。

### 子类化 nn.Module

当我们想要构建模型来做更复杂的事情，而不仅仅是一层接着一层地应用时，我们需要放弃
nn.Sequential 运算带来的灵活性。PyTorch 允许我们在模型中通过子类化 nn.Module 来进行任何运算。

为了子类化 nn.Module，我们至少需要定义一个 forward()方法，该方法接收模块的输入并返
回输出。这就是我们定义模块计算的地方。这里的 forward()让我联想到在 5.5.1 小节中碰到的模
块需要自定义正向和反向传播。在 PyTorch 中，如果我们使用标准 torch 操作，自动求导将自动
处理反向传播。事实上，一个 nn.Module 并不会自带 backward()方法。

```python
#In[26]:
class Net(nn.Module):
 def __init__(self):
 super().__init__()
 self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
 self.act1 = nn.Tanh()
 self.pool1 = nn.MaxPool2d(2)
 self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)
 self.act2 = nn.Tanh()
 self.pool2 = nn.MaxPool2d(2)
 self.fc1 = nn.Linear(8 * 8 * 8, 32)
 self.act3 = nn.Tanh()
 self.fc2 = nn.Linear(32, 2)

 def forward(self, x):
  out = self.pool1(self.act1(self.conv1(x)))
  out = self.pool2(self.act2(self.conv2(out)))
  out = out.view(-1, 8 * 8 * 8) # 这里是之前所缺少的
  out = self.act3(self.fc1(out))
  out = self.fc2(out)
  return out
```

![20220927171904](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220927171904.png)

**分类网络的目标通常是压缩信息。从某种意义上说，我们从一
个具有大量像素的图像开始，然后将其压缩到（一个概率向量的）
类别中。关于这个目标，我们的架构有 2 件事值得说明一下。**

首先，我们的目标是通过中间值的大小来反映的，通常收缩
是通过减少卷积中的通道数，通过池化减少像素的数量，以及通
过在线性层中使输出维度低于输入维度来实现的。这是分类网络的一个共同特征。然而，在许
多流行的架构中，如我们在第 2 章中看到并在 8.5.3 小节中讨论的 ResNet，_收缩是通过空间分
辨率池化实现的，但通道数量增加了（仍然导致维度大小减少）。快速信息收缩模式似乎适用
于深度有限、图像较小的网络，但对于较深的网络，收缩速度通常较慢。_

其次，在一层中，相对输入大小（初始卷积），输出大小并没有减少。如果我们将一个输出
像素看作有 32 个元素（通道）的向量，那么它是 27 个元素的线性变换（3 个通道核大小为 3×3
的卷积），只是有适度的增加。在 ResNet 中，初始卷积从 147 个元素（3 个通道核大小为 7×7）（**由第 1 次卷积定义的像素级线性映射中的维度**）
产生 64 个通道。因此，第 1 层的特殊之处在于它极大地增加了流经它的数据的整体维度（如通
道乘像素），但独立地考虑每个输出像素的映射仍然具有大约与输入一样的输出。（**在深度学习之外，比深度学习更古老的机器学习，投射到高维空间，然后在概念上做更简单(比线性)的
机器学习，通常被称为核技巧。最初通道数量的增加被看作一个与之有点儿类似的现象，但在嵌入的巧
妙性和处理嵌入的模型的简单性之间取得了不同的平衡。**）

### 函数式 API

事实上，torch.nn.function
提供了许多与 nn 中的模块类似的函数，但是它们不像模块那样处理输入实参和存储参数，而是
将输入和参数作为函数调用的实参。

例如，与 nn.Linear 对应的是 nn.functional.linear()，它是一个签名为 linear(input,weight,bias=
None)的函数，权重和偏置参数是函数的参数。

```python
# In[28]:
import torch.nn.functional as F
class Net(nn.Module):
 def __init__(self):
 super().__init__()
 self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
 self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)
 self.fc1 = nn.Linear(8 * 8 * 8, 32)
 self.fc2 = nn.Linear(32, 2)
def forward(self, x):
 out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)
 out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)
 out = out.view(-1, 8 * 8 * 8)
 out = torch.tanh(self.fc1(out))
 out = self.fc2(out)
 return out
```

由此，函数化的方式也揭示了 nn.Module API 的内涵：模块是状态的容器，将参数和子模块
的形式与指令相结合来完成转发。

**_是使用函数式 API 还是使用模块化 API 取决于开发者编码风格和具体使用场景体验。当网络
的一部分比较简单，以至于我们想要使用 nn.Sequential 时，我们使用模块化的 API。当我们编写自
己的 forward()方法时，对于不需要以参数形式表示状态的内容，使用函数式接口可能会更自然。_**

```python
# In[29]:
model = Net()
model(img.unsqueeze(0))
# Out[29]:
tensor([[-0.0157, 0.1143]], grad_fn=<AddmmBackward>)
```

### 训练我们的 convnet

同时，我们还收集和输出一些信息。这就是我们的训练循环，看起来几乎和第 7 章一样，但
是记住每个步骤是做什么的是有好处的。

```python
# In[30]:
import datetime
def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):
 for epoch in range(1, n_epochs + 1):
  loss_train = 0.0
  for imgs, labels in train_loader:
   outputs = model(imgs)
   loss = loss_fn(outputs, labels)
   optimizer.zero_grad()
   loss.backward()
   optimizer.step()
   loss_train += loss.item()
  if epoch == 1 or epoch % 10 == 0:
   print('{} Epoch {}, Training loss {}'.format(
   datetime.datetime.now(), epoch,
   loss_train / len(train_loader)))
```

我们使用第 7 章中的 Dataset，将其包装到 DataLoader 中，像以前一样实例化我们的网络、
优化器和损失函数，并调用我们的训练循环。

```python
# In[31]:
train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,
 shuffle=True)
model = Net() #
optimizer = optim.SGD(model.parameters(), lr=1e-2) #
loss_fn = nn.CrossEntropyLoss() #
training_loop(
 n_epochs = 100,
 optimizer = optimizer,
 model = model,
 loss_fn = loss_fn,
 train_loader = train_loader,
)
# Out[31]:
2020-01-16 23:07:21.889707 Epoch 1, Training loss 0.5634813266954605
2020-01-16 23:07:37.560610 Epoch 10, Training loss 0.3277610331109375
2020-01-16 23:07:54.966180 Epoch 20, Training loss 0.3035225479086493
2020-01-16 23:08:12.361597 Epoch 30, Training loss 0.28249378549824855
2020-01-16 23:08:29.769820 Epoch 40, Training loss 0.2611226033253275
2020-01-16 23:08:47.185401 Epoch 50, Training loss 0.24105800626574048
2020-01-16 23:09:04.644522 Epoch 60, Training loss 0.21997178820477928
2020-01-16 23:09:22.079625 Epoch 70, Training loss 0.20370126601047578
2020-01-16 23:09:39.593780 Epoch 80, Training loss 0.18939699422401987
2020-01-16 23:09:57.111441 Epoch 90, Training loss 0.17283396527266046
2020-01-16 23:10:14.632351 Epoch 100, Training loss 0.1614033816868712
```

### todo
