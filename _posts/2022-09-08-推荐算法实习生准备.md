---
layout: post
title: 推荐算法实习生准备
categories: 深度学习
---

## 前深度学习时代
### 传统的推荐模型演化关系图
![20220908212329](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220908212329.png)

### 阐述下你知道的神经网络结构，以及它们的异同点
首先AlexNet引入了多层卷积核堆叠的结构，这个结构用11\*11，5\*5，3\*3的卷积核堆叠而成，预测准确率很高。  
然后VGG提出了可以用多用3\*3的卷积核代替更大的卷积核，比如5\*5的卷积核可以用2个3\*3的卷积核代替，7\*7的卷积核可以用3个3\*3的卷积核代替，这样可以减少参数，同时让网络更深。同时VGG还给出了卷积神经网络模块的基本范式，即卷积-正则化-激活（Conv-BN-ReLU）的结构，被后面所沿用。  
之后，Inception Net提出了Bottleneck结构，即针对卷积神经网络层数较深的时候，输入的通道变得很多，此时参数量会大大膨胀，提出了可以把3\*3的卷积拆成1\*1与3\*3卷积的结合，第一个1\*1的卷积负责进行通道整合，第二个3\*3的卷积负责特征匹配。这种结构解决了深层卷积神经网络的参数膨胀问题，例如输入通道为c_in，输出通道为c_out的情况，如果直接卷积，需要的参数量为`c_in*3*3*c_out`，而采用Bottleneck结构，令1\*1卷积将c_in通道变成c_middle通道，则参数变成了c_in*c_middle+c_middle*3*3*c_cout，当输入通道很大的时候，这种结构能够减少参数量，而且可以进行特征聚合。  
基于Bottleneck结构，ResNet被提出了，它在卷积操作的同时引入了恒等映射作为跳层连接，解决了深层卷积神经网络退化的问题，这种做法也被扩展到Densenet使用。  
Shufflenet结合Bottleneck结构与分组卷积方式，提出的网络结构大大减少了参数量。分组卷积通过将输入通道为X的特征分为G组，每一组用相同的特征核，将参数量缩小了X/G倍。Shufflenet主要将分组卷积用在Bottlenet结构的1\*1卷积上，用1\*1，3\*3，1\*1的卷积堆叠代替直接进行卷积操作。  
与此同时，MobileNet提出了深度可分离卷积，简单来说就是把输入的特征看成一个group，用3\*3的分组卷积进行深度上的特征聚合，然后再用1\*1的卷积进行特征通道加权。  

### batch、epoch、iteration的关系

### 有哪些规范化操作，它们各自有什么用
Batch Normalization，Layer Normalization，Instance Normalization，Group Normalization。
Batch Normalization：针对一个Batch，在同一维度的特征进行feature scaling。
Layer Normalization：单独对一个样本的所有单词作缩放，与batch normalization的方向垂直，对RNN作用明显。
Instance Normalization：一个batch，一个channel内做归一化。常常用于生成模型，生成模型希望生成的样本较为丰富，样本间差异较大，因此用个体的规范化代替BatchNorm。
Group Normalization：将channel方向分group，然后每个group内做归一化。与batchsize无关，不受其约束。

### CTR点击率、CPA按成果数计费、CVR转化率、ROI投资回报率
CVR：衡量CPA广告效果的指标  
ROI：通过投资而应返回的价值，它涵盖了企业的获利目标。

### 协同过滤CF
#### User CF

#### Item CF
这一现象揭示了协同过滤的天然缺陷一推荐结果的头部效应较明显，处理稀疏向量的能力弱。另外， 协同过滤仅利用用户和物品的交互信息， 无法有效地引人用户年龄、性别、 商品描述、 商品分类、 当前时间等一系列用户特征、 物品特征和上下文特征， 这无疑造成了有效信息的遗漏。  

#### 矩阵分解算法
期望为每一个用户和视频生成一个隐向量，将用户和视频定位到隐向量的表示空间上，距离相近的用户和视频表明兴趣特点接近，在推荐过程中，就应该把距离相近的视频推荐给目标用户。  
对矩阵进行矩阵分解的主要方法有三种：特征值分解(Eigen Decomposition)、奇异值分解（Singular Value Decomposition, SVD)和梯度下降（Gradient Descent)。其中，特征值分解只能作用于方阵，显然不适用于分解用户-物品矩阵。  
( 1 ) 奇异值分解要求原始的共现矩阵是稠密的。互联网场景下大部分用户的行为历史非常少，用户-物品的共现矩阵非常稀疏，这与奇异值分解的应用条件相悖。如果应用奇异值分解，就必须对缺失的元素值进行填充。  
( 2 ) 传统奇异值分解的计算复杂度达到了0(mn2)的级别[4]，这对于商品数量动辄上百万、用户数量往往上千万的互联网场景来说几乎是不可接受的。  
一般用梯度下降法

### 逻辑回归LR
逻辑回归将推荐问题看成一个分类问题，通过预测正样本的概率对物品进行排序。将推荐问题转换成了一个点击率预估的问题。  
但其局限性也是非常明显的：表达能力不强，无法进行特征交叉、特征筛选等一系列较为高级” 的操作，因此不可避免地造成信息的损失。  

### PLOT2模型-特征交叉的开始
POLY2 进行无选择的特征交叉—原本就非常稀疏的特征向量更加稀疏，导致大部分交叉特征的权重缺乏有效的数据进行训练，无法收敛。

### FM模型-隐向量特征交叉
与POLY2相比，其主要区别是用两个向量的内积取代了单一的权重系数。具体的说FM为每个特征学习了一个隐权重向量（latent vector）。在特征交叉式，使用两个特征隐向量的内积作为交叉特征的权重。
![20220909141012](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220909141012.png)

### FFM模型-引入特征域的概念
引入特征域感知这一概念，使模型的表达能力更强。  
![20220909141418](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220909141418.png)
与FM的区别在于隐向量的变化，意味着每个特征对应的不是唯一一个隐向量，而是一组隐向量。  
![20220909142205](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220909142205.png)
FM模型族利用交叉特征的思路可以引申到三阶特征交叉，甚至更高维的阶段。但由于组合爆炸问题的限制，三阶FM无论是权重数量还是训练复杂度都过高，难以在实际工程中实现。那么，如何突破二阶特征交叉的限制，进一步加强模型特征组合的能力，就成了推荐模型发展的方向。  

### GBDT模型
决策树组成的森林，学习的方式是梯度提升。  
![20220909142614](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220909142614.png)
GBDT作为集成模型，预测的方式是把所有子树的结果加起来。理论上，如果可以无限生成决策树，那么GBDT可以无限逼近由所有训练集样本组成的目标拟合函数，从而达到减小预测误差的目的。
GBDT是由多棵回归树组成的树林，后一棵树以前面树林的结果与真实结果的残差为拟合目标。每棵树生成的过程是一棵标准的回归树生成过程，因此回归树中每个节点的分裂是一个自然的特征选择的过程，而多层节点的结构则对特征进行了有效的自动组合，也就非常高效地解决了过去棘手的特征选择和特征组合的问题。

## 深度学习在推荐系统中的应用

![20220919104806](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220919104806.png)

主要的演变方向如下：
（1）改变深度网络的复杂程度，增加了深度神经网络的层数和结构复杂度。  
（2）改变特征交叉方式，改变了用户向量和物品向量互操作方式的NeuralCF，定义了多种特征向量交叉操作的PNN。  
（3）组合模型，这类模型主要是指wide&deep模型及其后续变种Deep&Cross、DeepFM等。  
（4）FM模型的深度学习演化版本，其中包括NFM、FNN、AFM。
（5）注意力机制与推荐模型的结合，DIN。
（6）序列模型与推荐模型的结合，DIEN。
（7）强化学习与推荐模型的结合，DRN。

### AutoRec-单隐层神经网络推荐模型
将自编码器的思想和协同过滤结合，提出了一种单隐层神经网络推荐模型。

### Deep Crossing模型-深度学习架构在推荐系统中的完整应用
特征可以分为三类：  
`todo`

( 1 ) 离散类特征编码后过于稀疏，不利于直接输人神经网络进行训练，如何解决稀疏特征向量稠密化的问题。  
( 2 ) 如何解决特征自动交叉组合的问题。  
( 3 ) 如何在输出层中达成问题设定的优化目标。  
Deep Crossing模型分别设置了不同的神经网络层来解决上述问题。如图3-6所示，其网络结构主要包括 4 层Embedding 层、 Stacking 层、 Multiple ResidualUnits 层和 Scoring 层。  
![20220909143808](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220909143808.png)
Stacking 层（ 堆叠层 ） 的作用比较简单， 是把不同的 Embedding特征和数值型特征拼接在一起， 形成新的包含全部特征的特征向量。  
Multiple ResidualUnits 层采用了多层残差网络( Multi-Layer Residual Network ) 作为 MLP 的具体实现。  
Scoring 层作为输出层， 就是为了拟合优化目标而存在的。 对于CTR 预估这类二分类问题， Scoring 层往往使用的是逻辑回归模型， 而对于图像分类等多分类问题， Scoring 层往往采用 softmax 模型。  
DeepCrossing模型中没有任何人工特征工程的参与，原始特征经Embedding后输入神经网络层，将全部特征交叉的任务交给模型。

### Wide&Deep模型-记忆能力和泛化能力的综合
是由单层的 Wide 部分和多层的 Deep 部分组成的混合模型。其中wide部分的作用是让模型具有较强的 “ 记忆能力” memorization ); Deep 部分的主要作用是让模型具有 “泛化能力” generalization ), 正是这样的结构特点， 使模型兼具了逻辑回归和深度神经网络的优点—能够快速处理并记忆大量历史行为特征， 并且具有强大的表达能力。  
