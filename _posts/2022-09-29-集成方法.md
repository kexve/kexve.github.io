---
layout: post
title: 集成方法
categories: 深度学习
extMath: true
---

## Bagging

Bagging（ bootstrap aggregating）是通过结合几个模型降低泛化误差的技术
(Breiman, 1994)。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子，被称为 **模型平均（model
averaging）**。采用这种策略的技术被称为集成方法。

假设我们有 k 个回归模型。假设每个模型在每个例子上的误差是 $\epsilon_i$，这个误差
服从零均值方差为 $E[\epsilon_i^2 ] = v$ 且协方差为 $E[\epsilon_i \epsilon_j] = c$ 的多维正态分布。通过所有集
成模型的平均预测所得误差是 $\frac{1}{k}\sum_i \epsilon_i$。 集成预测器平方误差的期望是：

$$
E[(\frac{1}{k} \sum_i \epsilon_i)^2]=\frac{1}{k^2}E[\sum_i(\epsilon_i^2+\sum_{j \neq i} \epsilon_i \epsilon_j)] \\
=\frac{1}{k}v+ \frac{k-1}{k}c
$$

在误差完全相关即 c=v 的情况下，没有任何帮助，在误差完全不相关即 c=0 的情况下，集成网络的平方误差的期望为$\frac{1}{k}v$

不同的集成方法以不同的方式构建集成模型。例如， 集成的每个成员可以使用
不同的算法和目标函数训练成完全不同的模型。 Bagging 是一种允许重复多次使用同
一种模型、训练算法和目标函数的方法。

神经网络能找到足够多的不同的解，意味着他们可以从模型平均中受益 (即使所
有模型都在同一数据集上训练)。 神经网络中随机初始化的差异、 小批量的随机选择、
超参数的差异或不同输出的非确定性实现往往足以使得集成中的不同成员具有部分
独立的误差。

不是所有构建集成的技术都是为了让集成模型比单一模型更加正则化。例如，一
种被称为 Boosting 的技术 (Freund and Schapire, 1996b,a) 构建比单个模型容量更
高的集成模型。通过向集成逐步添加神经网络， Boosting 已经被应用于构建神经网络
的集成(Schwenk and Bengio, 1998)。通过逐渐增加神经网络的隐藏单元， Boosting 也
可以将单个神经网络解释为一个集成。

## Boosting
