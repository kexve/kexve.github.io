---
layout: post
title: 集成方法
categories: 深度学习
extMath: true
---

<!-- ## Bagging

Bagging（ bootstrap aggregating）是通过结合几个模型降低泛化误差的技术
(Breiman, 1994)。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子，被称为 **模型平均（model
averaging）**。采用这种策略的技术被称为集成方法。

假设我们有 k 个回归模型。假设每个模型在每个例子上的误差是 $\epsilon_i$，这个误差
服从零均值方差为 $E[\epsilon_i^2 ] = v$ 且协方差为 $E[\epsilon_i \epsilon_j] = c$ 的多维正态分布。通过所有集
成模型的平均预测所得误差是 $\frac{1}{k}\sum_i \epsilon_i$。 集成预测器平方误差的期望是：

$$
E[(\frac{1}{k} \sum_i \epsilon_i)^2]=\frac{1}{k^2}E[\sum_i(\epsilon_i^2+\sum_{j \neq i} \epsilon_i \epsilon_j)] \\
=\frac{1}{k}v+ \frac{k-1}{k}c
$$

在误差完全相关即 c=v 的情况下，没有任何帮助，在误差完全不相关即 c=0 的情况下，集成网络的平方误差的期望为$\frac{1}{k}v$

不同的集成方法以不同的方式构建集成模型。例如， 集成的每个成员可以使用
不同的算法和目标函数训练成完全不同的模型。 Bagging 是一种允许重复多次使用同
一种模型、训练算法和目标函数的方法。

神经网络能找到足够多的不同的解，意味着他们可以从模型平均中受益 (即使所
有模型都在同一数据集上训练)。 神经网络中随机初始化的差异、 小批量的随机选择、
超参数的差异或不同输出的非确定性实现往往足以使得集成中的不同成员具有部分
独立的误差。

不是所有构建集成的技术都是为了让集成模型比单一模型更加正则化。例如，一
种被称为 Boosting 的技术 (Freund and Schapire, 1996b,a) 构建比单个模型容量更
高的集成模型。通过向集成逐步添加神经网络， Boosting 已经被应用于构建神经网络
的集成(Schwenk and Bengio, 1998)。通过逐渐增加神经网络的隐藏单元， Boosting 也
可以将单个神经网络解释为一个集成。

## Boosting -->

## 概述

RF、GBDT 和 XGBoost 都属于集成学习（Ensemble Learning），集成学习的目的是**通过结合多个基学习器的预测结果来改善单个学习器的泛化能力和鲁棒性**。

根据个体学习器的生成方式，目前的集成学习方法大致分为两大类：即**个体学习器之间存在强依赖关系、必须串行生成的序列化方法**，以及**个体学习器间不存在强依赖关系、可同时生成的并行化方法**；前者的代表就是**Boosting**，后者的代表是**Bagging 和“随机森林”**（Random Forest）。

## 随机森林 RF

提到随机森林，就不得不提 Bagging，Bagging 可以简单的理解为：**放回抽样，多数表决（分类）或简单平均（回归）,同时 Bagging 的基学习器之间属于并列生成，不存在强依赖关系**。

Random Forest（随机森林）是 Bagging 的扩展变体，它在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括 RF 包括四个部分：

1. 随机选择样本（放回抽样）
2. **随机选择特征**
3. 构建决策树
4. 随机森林投票（平均）。

随机选择样本和 Bagging 相同，随机选择特征是指在树的构建中，会从样本集的特征集合中随机选择部分特征，然后再从这个子集中选择最优的属性用于划分，这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），但是由于随机森林的‘平均’特性，会使得它的方差减小，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。

​ 在构建决策树的时候，**RF 的每棵决策树都最大可能的进行生长而不进行剪枝**；在对预测输出进行结合时，RF 通常对分类问题使用简单投票法，回归任务使用简单平均法。

RF 的重要特性是**不用对其进行交叉验证**或者使用一个独立的测试集获得无偏估计，它可以在内部进行评估，也就是说在生成的过程中可以对误差进行无偏估计，由于每个基学习器只使用了训练集中约 63.2%的样本，剩下约 36.8%的样本可用做验证集来对其泛化性能进行“包外估计”。

**RF 和 Bagging 对比**：RF 的起始性能较差，特别当只有一个基学习器时，随着学习器数目增多，随机森林通常会收敛到更低的泛化误差。随机森林的训练效率也会高于 Bagging，因为在单个决策树的构建中，Bagging 使用的是‘确定性’决策树，在选择特征划分结点时，要对所有的特征进行考虑，而随机森林使用的是‘随机性’特征数，只需考虑特征的子集。

### 优缺点：

随机森林的优点较多，简单总结：

在数据集上表现良好，训练速度快、预测准确度较高；

能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性；

容易做成并行化方法。

RF 的缺点：在噪声较大的分类或者回归问题上会过拟合。

## GBDT

​
提 GBDT 之前，谈一下 Boosting，Boosting 是一种与 Bagging 很类似的技术。不论是 Boosting 还是 Bagging，所使用的多个分类器类型都是一致的。但是在前者当中，不同的分类器是通过**串行训练**而获得的，每个新分类器都根据已训练的分类器的性能来进行训练。**Boosting 是通过关注被已有分类器错分的那些数据来获得新的分类器。**

由于 Boosting 分类的结果是基于所有分类器的加权求和结果的，因此 Boosting 与 Bagging 不太一样，**Bagging 中的分类器权值是一样的，而 Boosting 中的分类器权重并不相等，每个权重代表对应的分类器在上一轮迭代中的成功度。**

### 原理

​ 原始的 Boost 算法是在算法开始的时候，为每一个样本赋上一个权重值，初始的时候，大家都是一样重要的。在每一步训练中得到的模型，会使得数据点的估计有对有错，我们就在每一步结束后，增加分错的点的权重，减少分对的点的权重，这样使得某些点如果老是被分错，那么就会被“严重关注”，也就被赋上一个很高的权重。然后等进行了 N 次迭代（由用户指定），将会得到 N 个简单的分类器（basic learner），然后我们将它们组合起来（比如说可以对它们进行加权、或者让它们进行投票等），得到一个最终的模型。

​ GBDT 与传统的 Boosting 区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型,所以说，在 GradientBoost 中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的 Boosting 中关注正确错误的样本加权有着很大的区别。

在 GradientBoosting 算法中，关键就是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵 CART 回归树。

​ GBDT 的会累加所有树的结果，而这种累加是无法通过分类完成的，因此 GBDT 的树都是 CART 回归树，而不是分类树（尽管 GBDT 调整后也可以用于分类但不代表 GBDT 的树为分类树）。

一些特性

每次迭代获得的决策树模型都要乘以一个缩减系数，从而降低每棵树的作用，提升可学习空间。
每次迭代拟合的是一阶梯度。

### 优缺点

GBDT 的性能在 RF 的基础上又有一步提升，因此其优点也很明显，

它能灵活的处理各种类型的数据；

在相对较少的调参时间下，预测的准确度较高。

缺点：当然由于它是 Boosting，因此基学习器之前存在串行关系，难以并行训练数据。

## XGBoost

### 原理

​ XGBoost 是 GBDT 的一个变种，最大的区别是 xgboost 通过对目标函数做二阶泰勒展开，从而求出下一步要拟合的树的叶子节点权重（需要先确定树的结构），从而根据损失函数求出每一次分裂节点的损失减小的大小，从而根据分裂损失选择合适的属性进行分裂。

​ 这个利用二阶展开的到的损失函数公式与分裂节点的过程是息息相关的。先遍历所有节点的所有属性进行分裂，假设选择了这个 a 属性的一个取值作为分裂节点，根据泰勒展开求得的公式可计算该树结构各个叶子节点的权重，从而计算损失减小的程度，从而综合各个属性选择使得损失减小最大的那个特征作为当前节点的分裂属性。依次类推，直到满足终止条件。

### XGBoost 与 GBDT 区别

1. 传统 GBDT 以 CART 作为基分类器，xgboost 还支持线性分类器，这个时候 xgboost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。

2. 传统 GBDT 在优化时只用到一阶导数信息，xgboost 则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost 工具支持自定义代价函数，只要函数可一阶和二阶求导。

3. xgboost 在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的 score 的 L2 模的平方和。从 Bias-variance tradeoff 角度来讲，正则项降低了模型的 variance，使学习出来的模型更加简单，防止过拟合，这也是 xgboost 优于传统 GBDT 的一个特性。

4. Shrinkage（缩减），相当于学习速率（xgboost 中的 eta）。xgboost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把 eta 设置得小一点，然后迭代次数设置得大一点。（传统 GBDT 的实现也有学习速率）
5. 列抽样（column subsampling）。xgboost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是 xgboost 异于传统 gbdt 的一个特性。
6. 对缺失值的处理。对于特征的值有缺失的样本，xgboost 可以自动学习出它的分裂方向。对于在训练过程中遇到的缺失值，xgboost 将其分别归到左子树和右子树分别计算损失，选取较优的哪一个。如果在训练中没有缺失值，在预测时遇到缺失值，就默认分到右子树。
7. xgboost 工具支持并行。注意 xgboost 的并行不是 tree 粒度的并行，xgboost 也是一次迭代完才能进行下一次迭代的（第 t 次迭代的代价函数里包含了前面 t-1 次迭代的预测值）。xgboost 的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost 在训练之前，预先对数据进行了排序，然后保存为 block 结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个 block 结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
8. 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以 xgboost 还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

4.3 xgboost 正则化（预剪枝）
xgboost 的目标函数如下：

![20230313114000](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230313114000.png)

xgboost 中树节点分裂时所采用的公式：

![20230313114017](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230313114017.png)

这个公式形式上跟 ID3 算法、CART 算法是一致的，都是用分裂后的某种值减去分裂前的某种值，从而得到增益。为了限制树的生长，我们可以加入阈值，当增益大于阈值时才让节点分裂，上式中的 gamma 即阈值，它是正则项里叶子节点数 T 的系数，所以 xgboost 在优化目标函数的同时相当于做了预剪枝。另外，上式中还有一个系数 lambda，是正则项里 leaf score 的 L2 模平方的系数，对 leaf score 做了平滑，也起到了防止过拟合的作用，这个是传统 GBDT 里不具备的特性。

## lightGBM 与 XGBoost 区别

### 决策树算法

由于在决策树在每一次选择节点特征的过程中，要遍历所有的属性的所有取值并选择一个较好的。

XGBoost 使用的是 pre-sorted 算法（对所有特征都按照特征的数值进行预排序，在遍历分割点的时候用 O(data)的代价找到一个特征上的最好分割点），能够更精确的找到数据分隔点；（xgboost 的分布式实现也是基于直方图的，利于并行）

LightGBM 使用的是 histogram 算法（X 发给类似一种分桶算法），占用的内存更低，数据分隔的复杂度更低。

直方图优化参考：[开源 \| LightGBM：三天内收获 GitHub 1000 星](https://www.msra.cn/zh-cn/news/features/lightgbm-20170105)

而这两个算法的比较见参考文献[如何看待微软新开源的 LightGBM?](https://www.zhihu.com/question/51644470/answer/130946285)

### 决策树生长策略

XGBoost 采用的是 level-wise 生长策略，如 Figure 1 所示，能够同时分裂同一层的叶子，从而进行多线程优化，也好控制模型复杂度，不容易过拟合；但实际上 Level-wise 是一种低效的算法，它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。

![20230313114325](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230313114325.png)

LightGBM 采用 leaf-wise 生长策略，如 Figure 2 所示，每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环；但会生长出比较深的决策树，产生过拟合。因此 LightGBM 在 leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。

![20230313114342](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230313114342.png)

### 其他

直方图做差加速

一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。

lightgbm 支持直接输入 categorical 的 feature

在对离散特征分裂时，每个取值都当作一个桶，分裂时的增益算的是”是否属于某个 category“的 gain。类似于 one-hot 编码。

多线程优化
