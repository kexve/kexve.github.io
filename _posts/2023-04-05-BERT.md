
---
layout: post
title: BERT
categories: [BERT, 面试]
---

## BERT

1. pre-training
2. ddp
3. bidirectional
4. transformer
5. language understanding

### BERT模型

BERT全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。

### 从ChatGPT中学习

BERT（Bidirectional Encoder Representations from Transformers）是一种**基于Transformer**模型的**预训练**语言模型，由Google于2018年提出。与传统的单向语言模型不同，BERT可以**从左到右**和**从右到左**同时进行**预训练**，从而获得更全面的语言表示。在预训练阶段，BERT使用大量无标注的语料来学习通用的语言知识，并通过fine-tuning（微调）来适应特定的下游任务，如文本分类、命名实体识别等。
BERT预训练的核心思想是使用**Masked Language Model**（MLM）和**Next Sentence Prediction**（NSP）两种任务来学习语言表示。

其中，MLM任务是**随机遮盖输入句子中的一些词汇，并要求模型预测被遮盖的词汇**。NSP任务是**要求模型根据输入的两个句子判断它们是否是连续的语言段**。

## transformer又是什么呢

Transformer是一种基于**self-attention**机制的神经网络模型，由Google在2017年提出，用于自然语言处理和其他序列到序列的任务。**在传统的序列模型中，如RNN和LSTM，需要顺序处理序列中的每个元素**，而Transformer通过self-attention机制来**并行计算输入序列中所有元素之间的依赖关系**，从而更高效地处理长序列数据。

Transformer模型由编码器和解码器两个部分组成。编码器用于将**输入序列转换为隐藏表示**，而解码器则将**隐藏表示映射到输出序列**。编码器和解码器都由多个相同的模块堆叠而成，每个模块都由两个子层组成：**Multi-Head Attention**和**Feedforward Neural Networks**。

其中，Multi-Head Attention子层通过**计算输入序列之间的注意力权重，从而将每个元素的表示形式调整为它与其他元素的关系的加权和**。Feedforward Neural Networks子层则用于**对调整后的表示进行进一步的处理和转换**。

## self-attention又是什么呢

Self-attention机制是一种用于**计算序列中不同位置之间依赖关系的方法**，最初被引入到Transformer模型中。**传统的attention机制通常用于将序列中的不同位置的信息聚合起来，用于后续的任务**，而**self-attention机制则可以同时考虑序列中所有位置的信息**，从而更好地捕捉序列中不同位置之间的依赖关系。

Self-attention机制的输入包括一个序列中的所有元素的表示，通常是一个矩阵，其中每行对应一个元素的表示。它通过计算序列中每个位置与所有其他位置之间的注意力权重，从而将每个位置的表示重新加权求和得到一个新的表示。这个加权求和的过程可以表示为矩阵乘法和softmax函数的组合。

## attention是什么

Attention机制是一种机制，用于**在一个序列（如文本或图像）中选择相关部分并加以关注**。它最初在机器翻译中被引入，用于将输入序列的不同部分与输出序列的不同部分相对应。通过加权汇聚输入序列的不同部分，Attention机制可以捕捉到序列中不同部分之间的关联关系，从而对于不同的输入序列，可以输出对应的不同翻译结果。

在自然语言处理中，注意力机制通常被用来**计算输入序列中每个位置对于输出序列中每个位置的权重**，用于生成目标序列。比如在机器翻译任务中，输入序列是待翻译的源语言句子，而输出序列是翻译后的目标语言句子。Attention机制可以在编码器和解码器之间建立一种对齐机制，将输入序列的每个位置对应到输出序列的每个位置上。

而Self-Attention机制是Attention机制的一种特殊形式，也被称为Intra-Attention机制。它将**输入序列的不同部分关联起来，以便序列中的每个部分都能够被用来计算其它部分的表示**。在Self-Attention中，输入序列的每个元素都可以作为查询项（Q）、键项（K）和值项（V），并通过计算它们之间的相似度，得到一个表示它们之间关系的注意力矩阵，然后将注意力矩阵应用到输入序列的值项上，以获得每个元素的新表示。

相比于传统的Attention机制，Self-Attention可以处理序列中长距离的依赖关系，因为它**能够直接计算序列中所有元素之间的相似度，而不需要考虑元素之间的距离**。因此，在序列建模任务中，Self-Attention机制已成为一种非常有用的工具。同时，Self-Attention机制也可以用于图像等非序列数据的建模中，以**帮助捕捉不同部分之间的关联性**。

