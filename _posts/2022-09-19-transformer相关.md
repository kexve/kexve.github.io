---
layout: post
title: transformer相关
categories: 深度学习
---

## attention 和 self attention

Attention 函数的本质可以被描述为一个查询（query）到一系列（键 key-值 value）对的映射。  
![20220919150558](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220919150558.png)
