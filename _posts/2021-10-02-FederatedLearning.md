---
layout: post 
title: 联邦学习    

categories:  Knowledge   
---    

## 联邦学习概念 federated learning  

与joint learning联合学习不同, 它是指将不同来源的数据整合在一起用于*多任务建模训练*的场景中. 
与multitask learning多任务学习不同, 它是指*迁移学习*的一个子方向, 旨在有多个学习目标并部分公用数据的情况下, 尽量利用共有模型提高学习效果.   

## 联邦学习是如何保护隐私和数据安全的？

### 模型训练

在模型训练阶段，模型相关的信息可以在各方之间交换，但数据不能交换，因此各个站点上的数据将受到保护.  

### 模型推理

在模型推理阶段，训练好的联邦学习模型可以置于联邦学习系统的各参与方，也可以供多方共享。这是联邦学习的具体过程，也就是它的定义.

## 联邦学习的4种基本类型

横向联邦学习、纵向联邦学习、联邦迁移学习和联邦强化学习.

![NeatReader-1633231135806](https://cdn.jsdelivr.net/gh/kexve/img/blogImgNeatReader-1633231135806.png)

![](https://cdn.jsdelivr.net/gh/kexve/img/blogImgNeatReader-1633231141398.png)

![](https://cdn.jsdelivr.net/gh/kexve/img/blogImgNeatReader-1633231146414.png)

## 联邦学习的发展动机

1. 用户隐私和数据安全方面的需求
2. 为了最大化地利用云系统下终端设备的计算能力, 减少了通过原始数据与中央服务器通信的需要

## 人工智能面临的挑战

这些系统的训练都需要很大的数据量才能达到一个令人满意的性能水平, 我们能够获得的通常都是“小数据”，即这些数据要么规模较小，要么缺少标签或者部分特征数值等重要信息, 我们不得不面对难以桥接的数据孤岛. 
数据拥有者只允许这些数据保存在自己手中，进而会形成各自孤立的数据孤岛. 
人工智能产业面临数据困境的另一个原因是，各方协同分享处理大数据的益处并不明显, 整合带来的性能增益是如何在参与方中分配的也不能完全确定. 

## 联邦学习的挑战

参与方（例如，智能手机）和中央聚合服务器之间的通信链接可能是慢速并且不稳定的，来自不同参与方的数据可能会导致出现非独立同分布的情况，并且不同的参与方可能有数量不均的训练数据样本，这可能导致联邦模型产生偏差，甚至会使联邦模型训练失败。难以被认证身份，这使得联邦学习模型容易遭到恶意攻击。

## 联邦学习的主要着眼

提升安全性以及处理统计学上的难题

## 联邦学习的开源平台

1. Federated AI Technology Enabler（FATE）,微众银行人工智能项目组发起的一个开源项目
2. TensorFlow Federated（TFF）,联邦学习和其他计算方法在去中心化数据集上进行实验的开源框架
3. 等等等

## 面向隐私保护的机器学习系统（Privacy-Preserving Machine Learning，PPML）

机器学习中的三种主要攻击类型：

1. 完整性（Integrity）：出现检测错误，例如可能会将入侵点检测为正常（假阴性）。
2. 可用性（Availability）：出现分类错误（假阴性和假阳性），即系统会变成不可用的。这是比完整性攻击更宽泛的一种攻击类型。
3. 机密性（Confidentiality）：敏感信息（如训练数据或训练模型）会出现泄露。

## 隐私威胁模型

攻击可能的发生阶段：

1. 数据发布：特征推理攻击（Attribute-Inference Attacks）
2. 模型训练：重构攻击（Reconstruction Attacks）,重构数据提供者的原始数据，或者学习关于数据的更多信息.**重构攻击是联邦学习的主要隐私关注点**
3. 模型推理：向工程技术来获取模型的额外信息，以此实施模型反演攻击（Model Inversion Attacks）或成员推理攻击（Membership-Inference Attacks）

### 重构攻击

敌手的目标是在模型的训练期间抽取训练数据，或抽取训练数据的特征向量。

在联邦学习中，只将模型的权重更新和梯度信息与其他参与方共享，如果数据结构是已知的，梯度信息可能也会被利用，从而泄露关于训练数据的额外信息。应当避免使用存储显式特征值的机器学习模型，例如支持向量机（SVM）和k近邻（kNN）模型。

安全多方计算和同态加密可以被用来通过保护计算中间结果来抵御重构攻击。

**计算方只应当被授予对模型的黑盒访问权限。**

### 模型反演攻击

敌手的目的是从模型中抽取训练数据或训练数据的特征向量。

理论上，对于一个*N*维的线性模型，一个敌手可以通过*N*+1次查询来窃取整个模型的内容。该问题的形式化是从（x，hθ（x））中求解θ。

为了抵御模型反演攻击，应当向敌手暴露尽可能少的关于模型的信息。对模型的访问应当被限制为黑盒访问，模型输出同样应当受限。

**将预测的类别标签作为返回结果等；同态加密的贝叶斯神经网络**

### 成员推理攻击

敌手的目标是判断模型的训练集中是否包含特定的样本。敌手的目标是获知给定样本是否在模型的训练集中。

### 特征推理攻击

敌手出于恶意目的，将数据去匿名化或锁定记录的拥有者。在面对能够获取其他背景知识的强大敌手时，匿名化将会失效。

**可能的解决方法----群组匿名化隐私方法。**

## 攻击者和安全模型

现有研究工作设计两种类型的敌手：

1. 半诚实的（Semi-honest）敌手，遵守协议，但也会试图从接收到的信息中学习更多除输出以外的信息。
2. 恶意的（Malicious）敌手，不遵守协议，可以执行任意的攻击行为。

大多数的PPML研究都考虑了半诚实的敌手模型。主要原因是，在联邦学习中，诚实地遵守协议是对各方都有利的。另一个原因是，在密码学中，首先建立一个针对半诚实的敌手的安全协议是一种标准的方法，然后可以**通过零知识证明（zero-knowledge proof）对其进行加强，进而防御恶意的敌手的攻击**。



## 隐私保护技术

### 安全多方计算（SMPC，Secure Multi-Party Computation）

在安全多方计算中，目的是协同地从每一方的隐私输入中计算函数的结果，而不用将这些输入展示给其他方。

证明安全多方计算协议是安全的标准方法为仿真范式（simulation paradigm）。

安全多方计算能够通过三种不同的框架来实现：**秘密共享被广泛认为是安全多方计算的核心**

#### 不经意传输（Oblivious Transfer，OT）

Alice 拥有秘密 SA ，Bob 拥有秘密 SB 。Alice 和 Bob 想要交换秘密，要求两方都有可能得到秘密并且秘密拥有方不知道对方是否得到秘密。

#### 秘密共享（Secret Sharing，SS）

秘密共享就是指共享的秘密在一个用户群体里进行合理分配，以达到由所有成员共同掌管秘密的目的。

### 安全多方运算在PPML中的应用

大多数基于安全多方计算的PPML方法利用两阶段架构，包括离线阶段和在线阶段。大多数密码操作都在离线阶段执行，在离线阶段生成乘法三元组。之后，于在线阶段使用离线阶段生成的乘法三元组对机器学习模型进行训练。

### 同态加密

使用者在不接触原有数据的情况下进行数据处理。

同态加密方法分为三类：部分同态加密（Partially Homomorphic Encryption，PHE），些许同态加密（Somewhat Homomorphic Encryption，SHE）和全同态加密（Fully Homomorphic Encryption，FHE）。

### 差分隐私

差分隐私使用来防范差分攻击的，差分攻击是指：

> 举个简单的例子，假设现在有一个婚恋数据库，2个单身8个已婚，只能查有多少人单身。刚开始的时候查询发现，2个人单身；现在张三跑去登记了自己婚姻状况，再一查，发现3个人单身。所以张三单身。

差分隐私需要做到的就是使得攻击者的知识不会因为这些新样本的出现而发生变化。例如对查询的结果加入噪声，使得攻击者无法辨别某一样本是否在数据集中。

在联邦学习中，为了使各方能在各自的分散数据集上进行模型训练，可以使用本地差分隐私（Local Differential Privacy，LDP）。

## 分布式机器学习（Distributed Machine Learning，DML）

联邦学习和分布式机器学习（Distributed Machine Learning，DML）有许多共同之处,很多研究者也把联邦学习看作分布式机器学习的一种特殊形式,或者把联邦学习看成是分布式机器学习的下一步发展。

DML可以分为两类：面向扩展性的DML（Scalability-Motivated DML）和面向隐私保护的DML（Privacy-Motivated DML）。面向扩展性的DML方法便为大规模ML提供了可行的解决方案，面向扩展性的DML被广泛应用于**具有横向划分数据集的场景中**。面向隐私保护的DML的主要目的是保护用户隐私和数据安全,由不同参与方拥有的数据集可能具有不同的数据特征，所以实际中经常遇到的是**训练数据的纵向划分**。

### 面向扩展型的DML

DML是大规模ML的一部分.一些著名的面向扩展性的DML方案，包括数据并行、模型并行、图并行、任务并行、混合并行和交叉并行。

### 面向隐私保护的DML

对于隐私保护的ML系统，它通常能保护下列的信息：训练数据输入、预测标签输出、模型信息（包括模型参数、结构和损失函数）和身份识别信息（如记录的数据来源站点、出处或拥有者）。

随着基于隐私的决策树方法的发展，人们开始考虑数据划分和利用隐私保护工具。

常用的用于保护数据隐私的方法大概分为两个类别：

1. 模糊处理（Obfuscation）：随机化、添加噪声或修改数据使其拥有某一级别的隐私，如差分隐私方法。
2. 密码学方法：通过不将输入值传给其他参与方的方式或者不以明文方式传输，使分布式计算过程安全化，如安全多方计算（MPC），包括不经意传输、秘密共享、混淆电路和同态加密。

总之，**由于计算效率和实现的便捷性**，基于模糊的隐私保护方法在基于隐私的DML系统中被普遍使用。但是，随机扰动影响了数据精度和模型性能。在实践中，研究者们不得不在隐私保护和性能之间取舍。与基于扰动的方法相比，**密码学方法并不需要牺牲数据精度和模型性能，但是需要更多的额外计算**。

在DL领域里，最有代表性的工作是安全聚合（Secure Aggregation）方法。基于由谷歌发布的联邦平均算法FedAvg（Federated Averaging），使用了FedAvg中的秘密共享、不经意传输，并考虑了在一个沟通成本高昂、客户加入退出频繁的复杂移动环境下的使用。

### 面向隐私保护的梯度下降算法

梯度下降方法是机器学习中的核心算法之一，越来越多的面向隐私保护的梯度下降方法正在被广泛研究。

典型的面向隐私保护的梯度下降方法包括朴素联邦学习（Naive Federated Learning或者Vanilla Federated Learning）、代数方法、稀疏梯度更新方法、模糊处理方法和密码学方法（如同态加密和安全多方计算）。



虽然DML在过去的几年中受到了广泛的关注，并且已经快速发展为开源和商业产品，但仍然存在现有DML系统无法解决的实际挑战。联邦学习是DML的一种特殊类型，它可以进一步解决传统DML系统面临的问题，如数据孤岛难题，并使我们能够构建面向隐私保护的人工智能系统和产品。

## 横向联邦学习

横向联邦学习也称为按样本划分的联邦学习（Sample-Partitioned Federated Learning或Example-Partitioned Federated Learning）。

之前的一些研究工作大部分都没有提供安全性的证明。另一种考虑了恶意用户的安全模型也被提出，这带来了联邦学习新的安全挑战。

### 横向联邦学习架构

#### 客户-服务器架构

![NeatReader-1633332687104](https://cdn.jsdelivr.net/gh/kexve/img/blogImgNeatReader-1633332687104.png)

联邦平均算法（Federated Averaging，FedAvg）：

1. 梯度平均（gradient averaging）：参与方将梯度信息发送给服务器，服务器将收到的梯度信息进行聚合（例如，计算加权平均），再将聚合的梯度信息发送给参与方。
2. 模型平均（model averaging）：参与方在本地计算模型参数，并将它们发送至服务器，服务器对收到的模型参数进行聚合（例如，计算加权平均），再将聚合的模型参数发送给参与方。

如果联邦平均算法**使用了安全多方计算或加法同态加密技术，这种架构便能防范半诚实的（semi-honest）服务器的攻击**，并防止数据泄露。在协同学习过程中，若有一个恶意的参与方**训练生成对抗网络（Generative Adversarial Network，GAN），将可能导致系统容易遭受攻击**。

#### 对等网络架构

![NeatReader-1633420951412](https://cdn.jsdelivr.net/gh/kexve/img/blogImgNeatReader-1633420951412.png)

训练方们必须提前商定发送和接收模型参数信息的顺序，

1. 循环传输（cyclic transfer）：

2. 随机传输（random transfer）：这种方法也叫作Gossip学习。

由于没有中央服务器，权重参数不分批量更新，而是连续更新，导致训练模型耗费更多的时间。

#### 全局模型评估

对于对等网络架构，由于不存在中央协调方或者中央服务器，要得到全局模型性能将会更为复杂。我们推荐将这种方法用于在训练结束之后评估最终的横向联邦学习模型的性能。

### 联邦平均算法，并行重启的随机梯度下降算法（parallel restarted SGD）

联邦学习中的优化问题被称为联邦优化，

1. 数据集的非独立同分布（Non-independent and Identically Distributed，Non-IID）

2. 不平衡的数据量

3. 数量很大的参与方

4. 慢速且不稳定的通信连接

对于许多模型而言，计算代价相比通信代价是微乎其微的。**可能需要使用额外的计算，以减少训练模型所需的通信轮次**（communication rounds）。

1. 增加并行度：加入更多的参与方
2. 增加每一个参与方中的计算

#### 联邦平均算法

对于一般的非凸目标函数，在模型参数空间中的模型平均可能会产生一个很差的联邦模型，甚至可能导致模型不能收敛。最近的研究表明，**充分参数化的DNN的损失函数表现得很好**，特别是出现不好的局部极小值的可能性比以前认为的要小。

**Dropout训练方法的成功经验为联邦模型平均方法提供了一些直观的经验解释。**Dropout训练可以被理解为在不同的共享模型参数的架构中的平均模型

#### 安全的联邦平均算法

例如可以用加法同态加密（AHE），具体如Paillier算法，或者基于带错误学习（Learning With Errors，LWE）的加密方法，来加强联邦平均算法的安全属性。

然而，加密操作和解密操作将会提高计算的复杂度，并且密文的传输也会增加额外的通信开销。AHE的另一个缺点是，为了评估非线性函数，需要使用多项式近似（例如，使用泰勒级数展开来近似计算损失函数和模型梯度）。**在精度与隐私性之间需要进行权衡。用于保护联邦平均算法的安全技术仍需进一步研究。**

### 联邦学习平均算法的改进

研究者提出了一些改善通信效率的方法，

1. 压缩的模型参数更新（Sketched updates）

2. 结构化的模型参数更新（Structured updates）

参与方选择，

1. 资源检查
2. 协调方使用这些信息估计

### 挑战与展望

1. 无法查看或者检查分布式的训练数据。这导致了我们面对的主要问题之一，就是很难选择机器学习模型的超参数以及设定优化器

