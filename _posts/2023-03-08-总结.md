---
layout: post
title: pytorch 框架
categories: [pytorch]
---

## 总览

PyTorch 是一个以 Python 为前端的框架，在借助 Python 这个动态语言本身的特性基础上使用动态图来定义模型，可以说 PyTorch 很好地借助了 Python 编译器的一些特性（例如垃圾回收，语言本身的动态性等），这使得用户在使用时会觉得好像比 tensorflow 这样使用静态图的框架容易。

结构上大约有两部分，一部分是 Python，另外一部分是 C/C++ 写的后端。Python 的部分比较容易读，如果后面有时间再介绍。C/C++ 后端的多维数组库在 v0.4 开始用 aten(a tensor library) 库作为上层封装再通过 torch/csrc 中的部分胶水代码接入 Python（但慢慢在改为 pybind11）。自动微分使用 C++完成，后端采用的是一个类似于录像带（tape）的机制来记录相关操作。新加入的 jit 部分还有详细读，不介绍。用`tree`看一下大约是这样的。后端核心代码主要在 lib 里。

## 动态图 eager mode 和静态图

### 动态图的初步推导

计算图是用来描述运算的有向无环图。

计算图有两个主要元素：结点（Node）和边（Edge）；

结点表示数据 ，如向量、矩阵、张量;

边表示运算 ，如加减乘除卷积等；

![20230308142009](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230308142009.png)

上图是用计算图表示：

![20230308142201](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230308142201.png)

现在我们用 Pytorch 的代码来实现这个过程：

```python
import torch
w = torch.tensor([1.],requires_grad = True)
x = torch.tensor([2.],requires_grad = True)

a = w+x
b = w+1
y = a*b

y.backward()
print(w.grad)
```

得到的结果：

![20230308142307](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230308142307.png)

### 动态图的叶子节点

这个图中的叶子节点，是 w 和 x，是整个计算图的根基。之所以用叶子节点的概念，是为了减少内存，在反向传播结束之后，非叶子节点的梯度会被释放掉，我们依然用上面的例子解释：

```python
import torch
w = torch.tensor([1.],requires_grad = True)
x = torch.tensor([2.],requires_grad = True)

a = w+x
b = w+1
y = a*b

y.backward()
print(w.is_leaf,x.is_leaf,a.is_leaf,b.is_leaf,y.is_leaf)
print(w.grad,x.grad,a.grad,b.grad,y.grad)
```

运行结果是：

![20230308142516](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230308142516.png)

可以看到只有 x 和 w 是叶子节点，然后反向传播计算完梯度后（.backward()之后），只有叶子节点的梯度保存下来了。

当然也可以通过.retain_grad()来保留非任意节点的梯度值。

### 静态图

两者的区别用一句话概括就是：

动态图：pytorch 使用的，运算与搭建同时进行；灵活，易调节。

静态图：老 tensorflow 使用的，先搭建图，后运算；高效，不灵活。

静态图我们是需要先定义好运算规则流程的。比方说，我们先给出

![20230308142810](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230308142810.png)

然后把上面的运算流程存储下来，然后把 w=1，x=2 放到上面运算框架的入口位置进行运算。而动态图是直接对着已经赋值的 w 和 x 进行运算，然后变运算变构建运算图。

在一个[课程](http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture08.pdf)中的第125页，有这样的一个对比例子：

![20230308142848](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230308142848.png)

这个代码是 Tensorflow 的，构建运算的时候，先构建运算框架，然后再把具体的数字放入其中。整个过程类似于训练神经网络，我们要构建好模型的结构，然后再训练的时候再把数据放到模型里面去。又类似于在旅游的时候，我们事先定要每天的行程路线，然后每天按照路线去行动。

![20230308143205](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230308143205.png)

动态图呢，就是直接对数据进行运算，然后动态的构建出运算图。很符合我们的运算习惯。

两者的区别在于，静态图先说明数据要怎么计算，然后再放入数据。假设要放入 50 组数据，运算图因为是事先构建的，所以每一次计算梯度都很快、高效；动态图的运算图是在数据计算的同时构建的，假设要放入 50 组数据，那么就要生成 50 次运算图。这样就没有那么高效。所以称为动态图。

动态图虽然没有那么高效，但是他的优点有以下：

1. 更容易调试。
2. 动态计算更适用于自然语言处理。（这个可能是因为自然语言处理的输入往往不定长？）
3. 动态图更面向对象编程，我们会感觉更加自然。

## 什么是 JIT

断断续续接触这两个概念有很长一段时间了，但是始终觉得对这两个 pytorch 的重要特性的概念就是比较模糊，中间还夹杂了一个 JIT trace 的概念，让我一句话归纳总结它们就是：

Eager 模式：Python + Python runtime。这种模式是更 Pythonic 的编程模式，可以让用户很方便的使用 python 的语法来使用并调试框架，就像我们刚认识 pytorch 时它的样子，自带 eager 属性。（但是我始终对这个 eager 有点对不上号 T \_ T）

Script 模式：TorchScript + PyTorch JIT。这种模式会对 eager 模式的模型创建一个中间表示（intermediate representation，IR），这个 IR 经过内部优化的，并且可以使用 PyTorch JIT 编译器去运行模型，不再依赖 python runtime，也可以使用 C++ 加载运行。

### script model

PyTorch 深受人们的喜爱主要是因为它的灵活和易用性（畏难心理，我到现在都还是对 TF 有点排斥），但是在模型部署方面，PyTorch 的表现却不尽人意，性能及可移植性都欠缺。之前使用 PyTorch 的痛点也是从研究到产品跨度比较大，不能直接将模型用来部署，为了解决这个 gap，PyTorch 提出了 TorchScript，想要通过它来实现从研究到产品的框架统一，通过 TorchScript 得到的模型可以脱离 python 的 runtime 并使你的模型跑的更快。

可移植性：script 模式可以不用再使用 python runtime，因此可以用在多线程推理服务器，移动设备，自动驾驶等 python 很难应用的场景。

性能表现：PyTorch JIT 是可以对 PyTorch 模型做特定优化的 JIT 编译器，其可以利用 runtime 的信息做量化，层融合，稀疏化等 Script 模型优化加速模型。

TorchScript 是一种编程语言，是 Python 的静态类型子集，它有自己的语法规则，我们使用 eager 模式来进行原型验证及训练的过程都是直接使用 python 语法，所以想得到方便部署的 script mode 需要通过 torch.jit.trace 或者是 torch.jit.script 去处理模型。

### 什么是 JIT

首先要知道 JIT 是一种概念，全称是 Just In Time Compilation，中文译为「即时编译」，是一种程序优化的方法，一种常见的使用场景是「正则表达式」。例如，在 Python 中使用正则表达式：

```python
prog = re.compile(pattern)
result = prog.match(string)
```

或

```python
result = re.match(pattern, string)
```

上面两个例子是直接从 Python 官方文档中摘出来的 ，并且从文档中可知，两种写法从结果上来说是「等价」的。但注意第一种写法种，会先对正则表达式进行 compile，然后再进行使用。如果继续阅读 Python 的文档，可以找到下面这段话：

> using re.compile() and saving the resulting regular expression object for reuse is more efficient when the expression will be used several times in a single program.

也就是说，如果多次使用到某一个正则表达式，则建议先对其进行 compile，然后再通过 compile 之后得到的对象来做正则匹配。而这个 compile 的过程，就可以理解为 JIT（即时编译）。

## 什么是 IR

## 翻译与执行

### 翻译层次

从翻译到执行可分为 4 个层次，没有包含 tracking。

1. binding：python 接口，自动求导，混合精度。
2. IR：自动类型转换，显存优化，代码生成。
3. Schedule：设备调度，资源分配。
4. compute：类型推导，计算实现，后端派发。

### 执行流程

整个 runtime 分为 3 个 stage：translation compilation Execution。

#### Translation

将 Python 编写的或 Autograd 产生的操作翻译到 Parrots IR 的形式。IR 表达的操作包括计算、流程控制以及其它指示性操作。

Opcode：IR 的基本单元，代表一个基本的操作，不具有独立的执行功能，只作为执行过程的一个步骤。比如一个 Tensor 的加法操作被翻译成一个 Opcode{'add'}。

Function：IR 中的函数，由一段 Opcode 序列和输入输出符号构成。是翻译、导出和编译的完整 IR 单元，可以独立重复执行，也可以生成对应的代码。通常可以对应到某种上层表述结构（如一个 Python 函数或者一个模型）。

call: 一个特殊的控制操作，表示调用一个 Function，JIT 可以通过它来调用翻译出的 Function。一个 Function 内部也可以包含 call 来调用其他 Function，形成递归调用。call + Function 构成了 IR 的分层嵌套结构。

inline: 直接将 Function 展开嵌入到立即执行操作流上， JIT 可以这样来内联执行翻译出的 Function。也可以将一个 Function 展开嵌入另一个 Function，形成更大的 Function，便于实现整体的优化。

JIT：在 Python 函数被调用时立即将其翻译到 IR，这产生一个完整的 Function。JIT 对 Python 函数有一定要求（比如无副作用）。

#### Compilation

将翻译出的 IR 整理变换成可执行的形式，其中可以插入代码优化和算子生成。

Optimizer：代码优化模块，对 Function 进行静态优化，可以有多个 pass，包括算子融合，死代码消除，内存释放顺序重排等。

Elena：算子编译器，可以根据输入的 Function 生成对应 CUDA 代码，代码可进一步编译成可执行 kernel。

#### Execution

执行编译得到的可执行的 IR，调用计算（通信等）模块得到实际的计算结果。

Interpreter：解释器，解释执行控制流和函数调用等操作，并将单纯的计算操作送给到 Scheduler 调度。

Executor：执行器，包含一个 Opcode 队列和执行线程 ，实现了同步和异步执行模式，即翻译过程和执行过程异步。

Scheduler：调度器，为计算操作的操作数分配设备存储空间（Storage），根据操作名字从算子库获取对应的算子实例（Operator），准备设备资源的上下文（Context），并执行算子。

Operators：所有的计算操作对应算子实现的集合，其中包含一个特殊的算子 CoderOp，它可以借助 RTC 技术将 CUDA 代码编译成可执行 kernel。可以编译 Elena 生成的代码并执行。

### IR 和引擎是如何组织的？如何支持静态优化？

组织方式如前所述，主要分 3 阶段。

翻译：将 Python 编写的或 Autograd 产生的操作翻译到 Parrots IR 的形式。IR 表达的操作包括计算、流程控制以及其它指示性操作。

编译：将翻译出的 IR 整理变换成可执行的形式，其中可以插入代码优化和算子生成。

执行：执行编译得到的可执行的 IR，调用计算（通信等）模块得到实际的计算结果。

静态优化以 Function 为基本单位。Function 由 JIT 产生。
静态优化的结果也是一个 Function，Function 可以被内联执行或调用执行。

parrots 内部的优化 pass 直接基于 Opcode 数据结构实现。
也可以将 Function 导出为 JSON 格式，在外部进行静态优化，再导入 parrots，例如 DLMO。

### 编译和 Elena 的结合方式？

Elena 可以看作一个静态变换，而其生成的 CUDA 代码可以在运行时再编译。

parrots 结合 Elena 的编译主要有两部分：

在编译层对特定 IR 做静态变换，将其生成为 Cuda 代码，CUDA 代码以 IR 为载体传入执行层。

在执行层引入一个特殊的算子 CoderOp，对 CUDA 代码进行编译，并调用编译出的 CUDA kernel。

细节步骤如下：

1. JIT 翻译需要做代码生成的函数，得到一个 Function（也可能是多个）。
2. 将一个 Function 中适合 Elena 编译的 opcode 段截取出来，目前的截取方式是直接在 Elena 不支持的算子处做分割。
3. 以截取出的一段 opcode 为例，将其升级包装成一个 sub Function，对 sub Fucntion 进行适的整理变换后，将其传入 Elena，
4. 从 elena 得到生成的代码等信息，将所有信息封装为一个 CoderOp 的 opcode，并嵌入回原来的 Function 中截取代码的位置。
5. 将 Function 送到执行层执行，在单个 Opcode 执行时，Scheduler 会调用对应的 CoderOp 实现。
6. CoderOp 从 opcode 中取出代码，利用 Nvida 的 RTC 技术将其编译成可执行 kernel。这里对编译生成的 kernel 做了缓存。
7. CoderOp 调用生成的 kernel，传入输入数据，在 CUDA 上执行实际的计算。
