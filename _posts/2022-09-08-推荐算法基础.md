---
layout: post
title: 推荐算法基础
categories: [深度学习,机器学习,面试]
---

## 常见的推荐系统模型

推荐系统是一种能够预测用户对物品的偏好度的系统，常见的推荐系统模型包括：

基于**内容**的推荐模型（Content-based Recommendation Model）：该模型使用物品的内容特征（例如电影的类型、演员、导演等）来推荐相似内容的物品给用户。

**协同过滤**推荐模型（Collaborative Filtering Recommendation Model）：该模型根据用户和物品之间的交互历史（例如用户购买、评价、收藏等）来推荐相似历史的物品给用户。协同过滤模型包括基于用户的协同过滤和基于物品的协同过滤。

**混合推荐**模型（Hybrid Recommendation Model）：该模型结合多种推荐模型，如基于内容的推荐模型和协同过滤推荐模型，以提高推荐系统的精度。

基于**矩阵分解**的推荐模型（Matrix Factorization Recommendation Model）：该模型将用户-物品交互矩阵分解成两个低维矩阵，从而获取用户和物品的隐藏特征，并通过这些特征预测用户对物品的偏好度。

**深度学习**推荐模型（Deep Learning Recommendation Model）：该模型使用深度神经网络来学习用户和物品之间的复杂关系，从而提高推荐系统的精度。

基于**序列**的推荐模型（Sequence-based Recommendation Model）：该模型考虑用户行为的序列（例如用户购买的顺序）来预测下一个用户行为，以此推荐合适的物品。

## 当今最先进的深度学习推荐模型
递归神经网络模型（Recurrent Neural Network，RNN）：RNN模型通过对用户行为序列进行建模，从而预测下一个用户行为。在推荐系统中，RNN模型可以用来预测用户下一次购买的物品或观看的视频等。

生成对抗网络模型（Generative Adversarial Network，GAN）：GAN模型通过对抗生成器和判别器来生成和推荐新的物品给用户。在推荐系统中，GAN模型可以用来生成用户可能感兴趣的新商品。

注意力机制模型（Attention Mechanism）：注意力机制模型可以从用户历史行为中自动地学习哪些历史行为对当前推荐任务更重要，从而提高推荐系统的精度。

深度协同过滤模型（Deep Collaborative Filtering Model）：深度协同过滤模型使用深度神经网络来建模用户和物品之间的交互历史，从而预测用户对物品的偏好度。

矩阵分解模型（Matrix Factorization Model）：矩阵分解模型使用深度神经网络来学习用户和物品的隐藏特征，并通过这些特征预测用户对物品的偏好度。矩阵分解模型在推荐系统中被广泛应用，如Google的Wide & Deep模型。

这些模型的具体实现方法和效果可能因应用场景和数据集而有所不同。

## 前深度学习时代

### 传统的推荐模型演化关系图

![20220908212329](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220908212329.png)

### 阐述下你知道的神经网络结构，以及它们的异同点

首先 AlexNet 引入了多层卷积核堆叠的结构，这个结构用 11\*11，5\*5，3\*3 的卷积核堆叠而成，预测准确率很高。
然后 VGG 提出了可以用多用 3\*3 的卷积核代替更大的卷积核，比如 5\*5 的卷积核可以用 2 个 3\*3 的卷积核代替，7\*7 的卷积核可以用 3 个 3\*3 的卷积核代替，这样可以减少参数，同时让网络更深。同时 VGG 还给出了卷积神经网络模块的基本范式，即卷积-正则化-激活（Conv-BN-ReLU）的结构，被后面所沿用。
之后，Inception Net 提出了 Bottleneck 结构，即针对卷积神经网络层数较深的时候，输入的通道变得很多，此时参数量会大大膨胀，提出了可以把 3\*3 的卷积拆成 1\*1 与 3\*3 卷积的结合，第一个 1\*1 的卷积负责进行通道整合，第二个 3\*3 的卷积负责特征匹配。这种结构解决了深层卷积神经网络的参数膨胀问题，例如输入通道为 c_in，输出通道为 c_out 的情况，如果直接卷积，需要的参数量为`c_in*3*3*c_out`，而采用 Bottleneck 结构，令 1\*1 卷积将 c_in 通道变成 c_middle 通道，则参数变成了 c_in*c_middle+c_middle*3*3*c_cout，当输入通道很大的时候，这种结构能够减少参数量，而且可以进行特征聚合。
基于 Bottleneck 结构，ResNet 被提出了，它在卷积操作的同时引入了恒等映射作为跳层连接，解决了深层卷积神经网络退化的问题，这种做法也被扩展到 Densenet 使用。
Shufflenet 结合 Bottleneck 结构与分组卷积方式，提出的网络结构大大减少了参数量。分组卷积通过将输入通道为 X 的特征分为 G 组，每一组用相同的特征核，将参数量缩小了 X/G 倍。Shufflenet 主要将分组卷积用在 Bottlenet 结构的 1\*1 卷积上，用 1\*1，3\*3，1\*1 的卷积堆叠代替直接进行卷积操作。
与此同时，MobileNet 提出了深度可分离卷积，简单来说就是把输入的特征看成一个 group，用 3\*3 的分组卷积进行深度上的特征聚合，然后再用 1\*1 的卷积进行特征通道权
重加权。

## 有哪些规范化操作，它们各自有什么用

Batch Normalization，Layer Normalization，Instance Normalization，Group Normalization。
Batch Normalization：针对一个 Batch，在同一维度的特征进行 feature scaling。
Layer Normalization：单独对一个样本的所有单词作缩放，与 batch normalization 的方向垂直，对 RNN 作用明显。
Instance Normalization：一个 batch，一个 channel 内做归一化。常常用于生成模型，生成模型希望生成的样本较为丰富，样本间差异较大，因此用个体的规范化代替 BatchNorm。
Group Normalization：将 channel 方向分 group，然后每个 group 内做归一化。与 batchsize 无关，不受其约束。

### CTR 点击率、CPA 按成果数计费、CVR 转化率、ROI 投资回报率

CVR：衡量 CPA 广告效果的指标
ROI：通过投资而应返回的价值，它涵盖了企业的获利目标。

### 协同过滤 CF

#### User CF

#### Item CF

这一现象揭示了协同过滤的天然缺陷一推荐结果的头部效应较明显，处理稀疏向量的能力弱。另外， 协同过滤仅利用用户和物品的交互信息， 无法有效地引人用户年龄、性别、 商品描述、 商品分类、 当前时间等一系列用户特征、 物品特征和上下文特征， 这无疑造成了有效信息的遗漏。

#### 矩阵分解算法

期望为每一个用户和视频生成一个隐向量，将用户和视频定位到隐向量的表示空间上，距离相近的用户和视频表明兴趣特点接近，在推荐过程中，就应该把距离相近的视频推荐给目标用户。
对矩阵进行矩阵分解的主要方法有三种：特征值分解(Eigen Decomposition)、奇异值分解（Singular Value Decomposition, SVD)和梯度下降（Gradient Descent)。其中，特征值分解只能作用于方阵，显然不适用于分解用户-物品矩阵。
( 1 ) 奇异值分解要求原始的共现矩阵是稠密的。互联网场景下大部分用户的行为历史非常少，用户-物品的共现矩阵非常稀疏，这与奇异值分解的应用条件相悖。如果应用奇异值分解，就必须对缺失的元素值进行填充。
( 2 ) 传统奇异值分解的计算复杂度达到了 0(mn2)的级别[4]，这对于商品数量动辄上百万、用户数量往往上千万的互联网场景来说几乎是不可接受的。
一般用梯度下降法

### 逻辑回归 LR

逻辑回归将推荐问题看成一个分类问题，通过预测正样本的概率对物品进行排序。将推荐问题转换成了一个点击率预估的问题。
但其局限性也是非常明显的：表达能力不强，无法进行特征交叉、特征筛选等一系列较为高级” 的操作，因此不可避免地造成信息的损失。

### PLOT2 模型-特征交叉的开始

POLY2 进行无选择的特征交叉—原本就非常稀疏的特征向量更加稀疏，导致大部分交叉特征的权重缺乏有效的数据进行训练，无法收敛。

### FM 模型-隐向量特征交叉

与 POLY2 相比，其主要区别是用两个向量的内积取代了单一的权重系数。具体的说 FM 为每个特征学习了一个隐权重向量（latent vector）。在特征交叉式，使用两个特征隐向量的内积作为交叉特征的权重。
![20220909141012](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220909141012.png)

### FFM 模型-引入特征域的概念

引入特征域感知这一概念，使模型的表达能力更强。
![20220909141418](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220909141418.png)
与 FM 的区别在于隐向量的变化，意味着每个特征对应的不是唯一一个隐向量，而是一组隐向量。
![20220909142205](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220909142205.png)
FM 模型族利用交叉特征的思路可以引申到三阶特征交叉，甚至更高维的阶段。但由于组合爆炸问题的限制，三阶 FM 无论是权重数量还是训练复杂度都过高，难以在实际工程中实现。那么，如何突破二阶特征交叉的限制，进一步加强模型特征组合的能力，就成了推荐模型发展的方向。

### GBDT 模型

决策树组成的森林，学习的方式是梯度提升。
![20220909142614](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220909142614.png)
GBDT 作为集成模型，预测的方式是把所有子树的结果加起来。理论上，如果可以无限生成决策树，那么 GBDT 可以无限逼近由所有训练集样本组成的目标拟合函数，从而达到减小预测误差的目的。
GBDT 是由多棵回归树组成的树林，后一棵树以前面树林的结果与真实结果的残差为拟合目标。每棵树生成的过程是一棵标准的回归树生成过程，因此回归树中每个节点的分裂是一个自然的特征选择的过程，而多层节点的结构则对特征进行了有效的自动组合，也就非常高效地解决了过去棘手的特征选择和特征组合的问题。

## 深度学习在推荐系统中的应用

![20220919104806](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220919104806.png)

主要的演变方向如下：
（1）改变深度网络的复杂程度，增加了深度神经网络的层数和结构复杂度。
（2）改变特征交叉方式，改变了用户向量和物品向量互操作方式的 NeuralCF，定义了多种特征向量交叉操作的 PNN。
（3）组合模型，这类模型主要是指 wide&deep 模型及其后续变种 Deep&Cross、DeepFM 等。
（4）FM 模型的深度学习演化版本，其中包括 NFM、FNN、AFM。
（5）注意力机制与推荐模型的结合，DIN。
（6）序列模型与推荐模型的结合，DIEN。
（7）强化学习与推荐模型的结合，DRN。

### AutoRec-单隐层神经网络推荐模型

将自编码器的思想和协同过滤结合，提出了一种单隐层神经网络推荐模型。

### Deep Crossing 模型-深度学习架构在推荐系统中的完整应用

特征可以分为三类：
`todo`

( 1 ) 离散类特征编码后过于稀疏，不利于直接输人神经网络进行训练，如何解决稀疏特征向量稠密化的问题。
( 2 ) 如何解决特征自动交叉组合的问题。
( 3 ) 如何在输出层中达成问题设定的优化目标。
Deep Crossing 模型分别设置了不同的神经网络层来解决上述问题。如图 3-6 所示，其网络结构主要包括 4 层 Embedding 层、 Stacking 层、 Multiple ResidualUnits 层和 Scoring 层。
![20220909143808](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20220909143808.png)
Stacking 层（ 堆叠层 ） 的作用比较简单， 是把不同的 Embedding 特征和数值型特征拼接在一起， 形成新的包含全部特征的特征向量。
Multiple ResidualUnits 层采用了多层残差网络( Multi-Layer Residual Network ) 作为 MLP 的具体实现。
Scoring 层作为输出层， 就是为了拟合优化目标而存在的。 对于 CTR 预估这类二分类问题， Scoring 层往往使用的是逻辑回归模型， 而对于图像分类等多分类问题， Scoring 层往往采用 softmax 模型。
DeepCrossing 模型中没有任何人工特征工程的参与，原始特征经 Embedding 后输入神经网络层，将全部特征交叉的任务交给模型。

### Wide&Deep 模型-记忆能力和泛化能力的综合

是由单层的 Wide 部分和多层的 Deep 部分组成的混合模型。其中 wide 部分的作用是让模型具有较强的 “ 记忆能力” memorization ); Deep 部分的主要作用是让模型具有 “泛化能力” generalization ), 正是这样的结构特点， 使模型兼具了逻辑回归和深度神经网络的优点—能够快速处理并记忆大量历史行为特征， 并且具有强大的表达能力。
