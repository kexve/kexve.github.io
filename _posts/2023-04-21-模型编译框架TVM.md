---
layout: post
titile: 模型编译推理框架TVM
categories: [推理框架, TVM]
---

## 前言

在任意深度学习的应用场景落地一个模型/算法时，需要经历两个基本步骤：

1. 根据数据生产一个模型的训练步骤；
2. 将生产出的模型部署到目标设备上执行服务的推理步骤。

**训练步骤目前基本由 Tensorflow、PyTorch、Keras、MXNet 等主流框架主导**，同样的，推理步骤目前也处在“百家争鸣”的状态。

## TVM 是什么？

TVM 是一款开源的、端到端的深度学习模型编译框架，用于**优化深度学习模型在 CPU、GPU、ARM 等任意目标环境下**的**推理运行速度**，常见的应用场景包括：

1. 需要兼容所有主流模型作为输入，并针对任意类型的目标硬件生成优化部署模型的场景
2. 对部署模型的推理延迟、吞吐量等性能指标有严格要求的场景
3. 需要自定义模型算子、自研目标硬件、自定义模型优化流程的场景

![20230421112139](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230421112139.png)

TVM 框架如上图：主流的深度学习框架（Tensorflow, Pytorch, MXNet 等）导出的模型作为 TVM 框架的输入，经过该框架内**一系列的图优化操作**以及**算子级的自动优化操作**后最终转化为针对目标运行时（CPU/GPU/ARM 等）的部署模型，优化后的模型理论上可以最大化地利用目标硬件的资源以最小化模型的推理延迟。

## 为什么用 TVM 优化模型推理？

模型推理场景下用于模型优化、部署的软件框架仍处于“百家争鸣”的状态，其原因在于**推理任务的复杂性**：训练后的模型需要**部署于多样的设备上（Intel CPU/ NVGPU/ ARM CPU/FPGA/ AI 芯片等）**，要在这些种类、型号不同的设备上都能保证模型推理的高效是一项极有挑战的工作。

一般来说，主流的**硬件厂商会针对自家硬件推出对应的推理加速框架以最大化利用硬件性能**，如 Intel 的 OpenVINO、ARM 的 ARM NN、Nvidia 的 TensorRT 等，但这些框架在实际应用场景中会遇到不少问题：

1. 厂商推理框架对主流训练框架产生的模型的算子种类支持不全，导致部分模型无法部署
2. 模型部署侧的开发人员需要针对不同的硬件编写不同的框架代码，花精力关注不同框架对算子的支持差异和性能差异等
3. 因此，一套可以让我们**在任意硬件上高效运行任意模型的统一框架**就显得尤其有价值，而 TVM 正是这样一套框架。

## TVM 如何优化模型推理？

实际上，“运行模型/代码到任意种类的硬件”并不是一个概念上全新的课题。在计算机编程语言发展的早期阶段（第二代编程语言），人们也曾遇到过类似的困境，即一种硬件平台必须配套一种汇编语言且不同汇编语言无法跨平台运行的情况。随着该领域的发展，人们给出了**解决之道——引入高级语言和编译器**，如下图：

![20230421112827](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230421112827.png)

1. 程序员负责用高级语言描述上层业务逻辑，不必关注具体硬件特性
2. 编译器将高级语言逐层转化为更底层的符号，也称中间表示（IR），其中最底层的 IR 可以对接不同的硬件，进而转化为针对不同目标的机器码

TVM 框架正是借鉴了这种思想，我们可以把 TVM 理解成一种广义的“编译器”：**TensorFlow、PyTorch 等训练框架导出的模型**可以认为是“高级语言”，而 TVM 内部的**图级别表达式树、算子级的调度 Stages**则可以认为是“高级语言”的“中间表示”，如下图。

![20230421113030](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230421113030.png)

## TVM 框架构成

以“模型部署”为边界，TVM 可以分为 TVM 编译器和 TVM 运行时两个组件：

![20230421113144](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230421113144.png)

编译器负责**模型的编译和优化**，是 TVM 的主体功能：

1. 编译优化过程支持 Python 和 C++接口
2. 系统环境支持 Linux、Windows 以及 MacOS 平台（部分功能如 **AutoTVM 在非 Linux 平台可能受限**）

运行时**负责在目标设备上执行编译器生成的模型推理代码**：

1. 部署过程支持 JS, Java, Python, C++语言
2. 部署平台除了支持 Linux、Windows 以及 MacOS 系统，还**支持 Android, IOS, 树莓派等端侧系统**

## TVM 编译过程

TVM 编译器是 TVM 的主体功能组件，负责**优化、编译深度学习模型为可在目标设备运行推理任务的代码**，整体编译过程如下图所示：

![20230421113702](https://cdn.jsdelivr.net/gh/kexve/img@main/image_blog20230421113702.png)

上图中，蓝色方框代表 TVM 编译过程中涉及到的数据结构，黄色方框代表处理这些数据结构的算法，粉色的 AutoTVM 则是辅助 Schedule 功能选择参数的一种特殊算法。

整个 TVM 编译栈的**数据结构**包括：

1. Model from Frameworks: 从 PyTorch、MXNet 等框架导出的模型
2. IR Module(Relay): TVM 的图级别 IR，数据结构为 Relay Expression 构成的 Relay AST，查看源码可以发现，relay.Function 实际上是 relay.Expr 的一种子类，其功能是为 Relay Pass（遍历 Relay AST 的手段）提供一个入口点，故而官网中也把 Relay IR 称为"End to end function"
3. IR Module(TIR): TVM 的 Tensor 级别 IR，仍然以 AST 的形式组织 Expr，但包含了各算子（对应网络层）的具体调度细节（循环嵌套、并行、指令集等），这一层 IR 由一系列的 tir.PrimFunc 组成，此处的 PrimFunc 不再是访问整个 AST 的“入口点”，而是对应神经网络的一个层或融合层（如 Conv+BN+Activation）的具体计算调度过程
4. Runtime Module: TVM 编译栈的最底层 IR，直接对接 Runtime 在目标设备上运行

TVM 编译栈涉及的**数据处理算法**包括：

1. Frontend: 负责将外部框架生成的神经网络模型转化为 TVM 的图级别模型 Relay IR
2. Relay Passes: 负责在图级别优化 IR 的计算过程，常见的图优化 Pass 包括常量折叠、死代码消除、推理简化、内存布局转化、Op 融合等
3. Scheduling: 负责转化 Relay Passes 优化后的 Relay IR 为 TensorIR，大致的流程是对于每个层/融合层对应的算子，执行 http://graph_runtime_codegen.cc 的 Codegen，根据 TVM 框架中注册的算子 compute 和 schedule 函数，将每个算子转化为具体的计算调度过程（Relay IR --> FTVMCompute --> Tensors --> FTVMSchedule --> Schedule Stages）
4. TIR Passes: 负责 Tensor 级别的 IR 优化，常见的如 access index simplification，也有负责 IR lower 的 function entry decoratation 等
5. Target Translation(CodeGen): 设备相关的低级别 Codegen，将 TIR 转化为 TVM Runtime 所需的 Module

## TVM 特色

### 特色 1：算子的计算和调度过程定义的分离

TVM 的一个特色是借鉴 **Halide 语言的计算和调度分离定义的设计方法**，将深度学习模型的算子计算方法定义和具体计算调度方式做了解耦：

1. 计算（Compute）定义了每个 Relay Op 的具体计算过程

2. 调度（Schedule）则细化了计算过程每个具体步骤的实际计算细节

以卷积算子为例，计算（Compute）定义的是算子应如何根据卷积核、卷积步长等参数操作输入 Tensor，并输出卷积后的 Tensor 的过程；调度（Schedule）则定义了更具体的计算细节，例如卷积的 6 层计算循环的顺序、每层循环是否需要并行计算、并行数设定为多少等。

之所以做这样的分离式设计，是因为一般情况下，**特定算子的计算方式是不变的，而算子的调度却有几乎无限种配置方法**，显然对于不同的 Target **如 CPU、GPU，其最优调度配置必然大相径庭**；即使对于同一种 Target 如 LLVM，不同型号的 CPU 也会导致最优配置相差甚远。TVM 的计算-调度分离的设计方式，给二次开发人员提供了足够便利的 API，方便其自定义性能更佳的调度配置。

### 特色 2：AutoTVM

默认情况下，**TVM 为定义的所有算子针对常见的 Target（x86/ARM/CUDA 等）各提供了一套默认的调度配置。**但一方面指定 Target 的默认配置不可能在所有型号的硬件下最大程度地利用运行环境的计算资源；另一方面调度的可选配置空间巨大，人工手动调整调度效率低且无法保证调度性能。为了解决该问题，TVM **开发了 AutoTVM 模块通过搜索的方式获得具体硬件下的最优调度配置**，总体步骤如：

1. 针对每个 Relay 子函数，**定义一系列调度原语**（Schedule primitives），通过调度原语的组合实现计算结果等价的 Relay 子函数
2. **在海量的配置组合中为每个 Relay 子函数搜索最优的调度配置**

对于卷积这样的计算密集算子，调度配置的搜索空间通常有几十亿种选择，比较棘手的问题就是如何高效的在搜索空间中获得最优/局部最优解。常见的做法有两种：

1. 遍历所有可能的参数编译模型并评估
2. 评估部分参数模型，**训练一个性能评估函数，用于指导搜索过程寻找更好的调度配置**

由于搜索空间过于巨大，第一种方法需要花费的时间难以估量；AutoTVM 采用的是第二种，用一个 **XGBoost 模型（其他回归预测模型也可）作为评估调度配置性能的模型**（CostModel），用以**指导模拟退火算法（默认）寻找最优调度配置**。

搜索过程完毕后 AutoTVM 会将过程中记录的最优配置保存下来，编译模型时可用于代替默认的 Target 调度配置。
